import os
import re
import json
import asyncio
import time
import logging
import hashlib
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, date, timedelta
from decimal import Decimal
from collections import deque, defaultdict
import aiohttp
import psycopg2
from textwrap import dedent
from psycopg2.extras import RealDictCursor
from collections import Counter, defaultdict
logger = logging.getLogger(__name__)

class ImprovedIntentDetector:
    """
    Enhanced Intent Detector - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    - Keywords ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á
    - Negative keywords ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    - Business domain awareness
    - Better confidence calculation
    """
    
    def __init__(self):
        # =================================================================
        # ENHANCED INTENT KEYWORDS (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å)
        # =================================================================
        self.intent_keywords = {
            'pricing': {
                'strong': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤', 'quotation', 'price', 'cost', 'quote'],
                'medium': ['Standard', '‡∏á‡∏≤‡∏ô', '‡∏™‡∏£‡∏∏‡∏õ', '‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'],  # ‡πÄ‡∏û‡∏¥‡πà‡∏° Standard!
                'weak': ['‡∏ö‡∏≤‡∏ó', '‡πÄ‡∏á‡∏¥‡∏ô', '‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢'],
                'patterns': [
                    r'‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤.*‡∏á‡∏≤‡∏ô',
                    r'‡∏á‡∏≤‡∏ô.*Standard',  # ‡πÄ‡∏û‡∏¥‡πà‡∏° pattern ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                    r'‡∏™‡∏£‡∏∏‡∏õ.*‡∏£‡∏≤‡∏Ñ‡∏≤',
                    r'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£.*‡∏£‡∏≤‡∏Ñ‡∏≤',
                    r'‡∏£‡∏≤‡∏Ñ‡∏≤.*‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏ä‡πà‡∏≤‡∏á', '‡∏ó‡∏µ‡∏°']  # ‡πÄ‡∏≠‡∏≤ '‡∏á‡∏≤‡∏ô' ‡∏≠‡∏≠‡∏Å
            },
            
            'sales': {
                'strong': ['‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', 'revenue', 'sales', 'income', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢'],
                'medium': ['overhaul', 'replacement', 'service', '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏á‡∏≤‡∏ô'],  # ‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤
                'weak': ['‡∏£‡∏ß‡∏°', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', 'total', '‡∏ö‡∏≤‡∏ó'],
                'patterns': [
                    r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏Ç‡∏≤‡∏¢',
                    r'‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ.*‡∏õ‡∏µ',
                    r'‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢.*‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                    r'‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢.*‡∏Ç‡∏≠‡∏á'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏ä‡πà‡∏≤‡∏á', '‡∏ó‡∏µ‡∏°', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']  # ‡πÄ‡∏≠‡∏≤ '‡∏á‡∏≤‡∏ô' ‡∏≠‡∏≠‡∏Å
            },
            
            'sales_analysis': {
                'strong': ['‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå', 'analysis', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô'],
                'medium': ['‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', 'revenue', '‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö'],
                'weak': ['‡∏õ‡∏µ', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡πà‡∏ß‡∏á'],
                'patterns': [
                    r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏Ç‡∏≤‡∏¢',
                    r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ',
                    r'‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö.*‡∏õ‡∏µ'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'overhaul_report': {
                'strong': ['overhaul', '‡πÇ‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏Æ‡∏≠‡∏•', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô'],
                'medium': ['compressor', '‡∏Ñ‡∏≠‡∏°‡πÄ‡∏û‡∏£‡∏™‡πÄ‡∏ã‡∏≠‡∏£‡πå', '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡∏ã‡πà‡∏≠‡∏°'],
                'weak': ['‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏á‡∏≤‡∏ô'],
                'patterns': [
                    r'overhaul.*compressor',
                    r'‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô.*overhaul',
                    r'‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢.*overhaul'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'work_force': {
                'strong': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏ä‡πà‡∏≤‡∏á', 'service_group', '‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô'],
                'medium': ['project', '‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£', 'success', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô'],
                'weak': ['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤'],
                'patterns': [
                    r'‡∏á‡∏≤‡∏ô.*‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                    r'‡∏ó‡∏µ‡∏°.*‡∏á‡∏≤‡∏ô',
                    r'‡∏á‡∏≤‡∏ô.*‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à',
                    r'‡∏á‡∏≤‡∏ô.*‡∏ó‡∏≥'
                ],
                'negative': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤']  # ‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤
            },
            
            'work_plan': {
                'strong': ['‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô', '‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô', 'plan', 'schedule', '‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£'],
                'medium': ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏á‡∏≤‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á', '‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á'],
                'weak': ['‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤', '‡∏ï‡πà‡∏≠‡πÑ‡∏õ'],
                'patterns': [
                    r'‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô.*‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                    r'‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô.*‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà',
                    r'‡∏á‡∏≤‡∏ô.*‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô',
                    r'‡πÅ‡∏ú‡∏ô.*‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á'
                ],
                'negative': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à']
            },
            
            'work_summary': {
                'strong': ['‡∏™‡∏£‡∏∏‡∏õ‡∏á‡∏≤‡∏ô', '‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥', 'summary'],
                'medium': ['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡πà‡∏ß‡∏á', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡πÄ‡∏™‡∏£‡πá‡∏à'],
                'weak': ['‡∏ú‡∏•‡∏á‡∏≤‡∏ô', '‡πÑ‡∏î‡πâ', '‡πÅ‡∏•‡πâ‡∏ß'],
                'patterns': [
                    r'‡∏™‡∏£‡∏∏‡∏õ.*‡∏á‡∏≤‡∏ô',
                    r'‡∏á‡∏≤‡∏ô.*‡∏ó‡∏µ‡πà‡∏ó‡∏≥',
                    r'‡∏á‡∏≤‡∏ô.*‡πÄ‡∏™‡∏£‡πá‡∏à'
                ],
                'negative': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'spare_parts': {
                'strong': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', 'spare', 'part', '‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô'],
                'medium': ['stock', '‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠', '‡∏Ñ‡∏•‡∏±‡∏á', '‡πÄ‡∏Å‡πá‡∏ö'],
                'weak': ['EK', 'model', 'HITACHI', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á'],
                'patterns': [
                    r'‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà.*‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á',
                    r'‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô.*model',
                    r'spare.*part'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'parts_price': {
                'strong': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', 'price'],
                'medium': ['‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á', 'model', '‡∏ó‡∏£‡∏≤‡∏ö', '‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ'],
                'weak': ['‡∏ö‡∏≤‡∏ó', '‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà', 'cost'],
                'patterns': [
                    r'‡∏£‡∏≤‡∏Ñ‡∏≤.*‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà',
                    r'‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà.*‡∏£‡∏≤‡∏Ñ‡∏≤',
                    r'‡∏ó‡∏£‡∏≤‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤.*‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á',
                    r'‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏£‡∏≤‡∏ö.*‡∏£‡∏≤‡∏Ñ‡∏≤'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ']
            },
            
            'inventory_value': {
                'strong': ['‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤', '‡∏Ñ‡∏á‡∏Ñ‡∏•‡∏±‡∏á', 'inventory', 'value'],
                'medium': ['‡∏™‡∏ï‡πá‡∏≠‡∏Å', '‡∏Ñ‡∏•‡∏±‡∏á', '‡πÄ‡∏Å‡πá‡∏ö', '‡∏£‡∏ß‡∏°'],
                'weak': ['‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', 'total'],
                'patterns': [
                    r'‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤.*‡∏Ñ‡∏á‡∏Ñ‡∏•‡∏±‡∏á',
                    r'‡∏™‡∏ï‡πá‡∏≠‡∏Å.*‡∏Ñ‡∏•‡∏±‡∏á'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤']
            },
            
            'customer_history': {
                'strong': ['‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', 'history', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó'],
                'medium': ['‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', 'customer', '‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢', '‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠'],
                'weak': ['‡πÄ‡∏Å‡πà‡∏≤', '‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤', '‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á'],
                'patterns': [
                    r'‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤',
                    r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó.*‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥',
                    r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢',
                    r'‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠.*‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà']
            },
            
            'repair_history': {
                'strong': ['‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', '‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°', '‡∏ã‡πà‡∏≠‡∏°', 'repair'],
                'medium': ['‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏≠‡∏∞‡πÑ‡∏£'],
                'weak': ['‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà', '‡πÄ‡∏Ñ‡∏¢'],
                'patterns': [
                    r'‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥.*‡∏ã‡πà‡∏≠‡∏°',
                    r'‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°.*‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó',
                    r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó.*‡∏ã‡πà‡∏≠‡∏°'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'top_customers': {
                'strong': ['‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', 'Top', '‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö', '‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î'],
                'medium': ['‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡∏´‡∏•‡∏±‡∏Å'],
                'weak': ['5', '10', 'best'],
                'patterns': [
                    r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*Top',
                    r'Top.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤',
                    r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°']
            }
        }

        # =================================================================
        # ENHANCED MONTH MAPPING
        # =================================================================
        self.month_map = {
            # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°
            '‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°': 1, '‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå': 2, '‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°': 3,
            '‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô': 4, '‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°': 5, '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô': 6,
            '‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°': 7, '‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°': 8, '‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô': 9,
            '‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°': 10, '‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô': 11, '‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°': 12,
            
            # ‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠
            '‡∏°.‡∏Ñ.': 1, '‡∏Å.‡∏û.': 2, '‡∏°‡∏µ.‡∏Ñ.': 3,
            '‡πÄ‡∏°.‡∏¢.': 4, '‡∏û.‡∏Ñ.': 5, '‡∏°‡∏¥.‡∏¢.': 6,
            '‡∏Å.‡∏Ñ.': 7, '‡∏™.‡∏Ñ.': 8, '‡∏Å.‡∏¢.': 9,
            '‡∏ï.‡∏Ñ.': 10, '‡∏û.‡∏¢.': 11, '‡∏ò.‡∏Ñ.': 12,
            
            # English
            'january': 1, 'february': 2, 'march': 3,
            'april': 4, 'may': 5, 'june': 6,
            'july': 7, 'august': 8, 'september': 9,
            'october': 10, 'november': 11, 'december': 12,
            
            'jan': 1, 'feb': 2, 'mar': 3,
            'apr': 4, 'jun': 6, 'jul': 7,
            'aug': 8, 'sep': 9, 'oct': 10,
            'nov': 11, 'dec': 12
        }

        # =================================================================
        # BUSINESS-SPECIFIC TERMS
        # =================================================================
        self.business_terms = {
            'hvac_equipment': [
                'chiller', '‡∏Ñ‡∏¥‡∏•‡πÄ‡∏•‡∏≠‡∏£‡πå', '‡πÅ‡∏≠‡∏£‡πå', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏®',
                'compressor', '‡∏Ñ‡∏≠‡∏°‡πÄ‡∏û‡∏£‡∏™‡πÄ‡∏ã‡∏≠‡∏£‡πå', 'AHU', 'FCU'
            ],
            'service_types': [
                'PM', 'maintenance', '‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤', 'overhaul', 
                'replacement', '‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô', '‡∏ã‡πà‡∏≠‡∏°', 'service'
            ],
            'brands': [
                'HITACHI', 'CLARION', 'EK', 'EKAC', 'RCUG', 
                'Sadesa', 'AGC', 'Honda'
            ]
        }

    # =================================================================
    # MAIN DETECTION METHOD
    # =================================================================
    
    def detect_intent_and_entities(self, question: str, 
                                previous_intent: Optional[str] = None) -> Dict[str, Any]:
        """
        Enhanced intent detection with better accuracy
        FIXED: Handle overhaul with sales/work context properly
        """
        question_lower = question.lower().strip()
        
        # ============================================
        # PRIORITY CHECKS (‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠)
        # ============================================
        
        # 1. CPA Work
        if 'cpa' in question_lower and '‡∏á‡∏≤‡∏ô' in question_lower:
            return {'intent': 'cpa_work', 'confidence': 0.95, 'entities': self._extract_entities(question, 'cpa_work')}
        
        # 2. PM Work
        if 'pm' in question_lower and '‡∏á‡∏≤‡∏ô' in question_lower:
            return {'intent': 'pm_work', 'confidence': 0.90, 'entities': self._extract_entities(question, 'pm_work')}
        
        # ============================================
        # üîß FIX: OVERHAUL CONTEXT DETECTION
        # ============================================
        if 'overhaul' in question_lower:
            # Check for SALES context FIRST (higher priority)
            sales_indicators = [
                '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', 'sales', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', 
                'revenue', '‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏ö‡∏≤‡∏ó', 'income'
            ]
            
            # Check for WORK context
            work_indicators = [
                '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô', '‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥', 'work', '‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£', 
                '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î', 'detail', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'date'
            ]
            
            # Priority 1: Sales context wins
            if any(word in question_lower for word in sales_indicators):
                return {
                    'intent': 'overhaul_sales',  # ‚úÖ Sales intent
                    'confidence': 0.95,
                    'entities': self._extract_entities(question, 'overhaul_sales')
                }
            
            # Priority 2: Explicit work context
            elif any(word in question_lower for word in work_indicators):
                return {
                    'intent': 'work_overhaul',  # Work intent
                    'confidence': 0.90,
                    'entities': self._extract_entities(question, 'work_overhaul')
                }
            
            # Priority 3: Just "‡∏á‡∏≤‡∏ô overhaul" without other context
            elif '‡∏á‡∏≤‡∏ô' in question_lower:
                # Default to work unless has "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô"
                if '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô' in question_lower:
                    return {
                        'intent': 'overhaul_sales',  # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô = sales report
                        'confidence': 0.85,
                        'entities': self._extract_entities(question, 'overhaul_sales')
                    }
                else:
                    return {
                        'intent': 'work_overhaul',
                        'confidence': 0.85,
                        'entities': self._extract_entities(question, 'work_overhaul')
                    }
            
            # Priority 4: Just "overhaul" alone - check other context
            else:
                # Default to sales for standalone "overhaul"
                return {
                    'intent': 'overhaul_sales',
                    'confidence': 0.80,
                    'entities': self._extract_entities(question, 'overhaul_sales')
                }
        
        # ============================================
        # OTHER SPECIFIC PATTERNS
        # ============================================
        
        # Employee queries
        employee_patterns = ['‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠', '‡∏ó‡∏µ‡∏°‡∏Ç‡∏≠‡∏á', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á']
        if any(pattern in question_lower for pattern in employee_patterns):
            entities = self._extract_entities(question, 'employee_work')
            if entities.get('employees'):
                return {
                    'intent': 'employee_work',
                    'confidence': 0.95,
                    'entities': entities
                }
        
        # Customer history
        if any(word in question_lower for word in ['‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', 'history', '‡πÄ‡∏Ñ‡∏¢', '‡∏ã‡πà‡∏≠‡∏°']):
            entities = self._extract_entities(question, 'customer_history')
            if entities.get('customers'):
                return {
                    'intent': 'customer_history',
                    'confidence': 0.90,
                    'entities': entities
                }
        
        # ============================================
        # NORMAL PROCESSING (existing logic)
        # ============================================
        
        # Preprocess question
        processed_question = self._preprocess_question(question_lower)
        
        # Calculate intent scores
        intent_scores = self._calculate_intent_scores(processed_question, previous_intent)
        
        # Get best intent with confidence
        best_intent, confidence = self._get_best_intent_with_confidence(intent_scores, processed_question)
        
        # Extract entities
        entities = self._extract_entities(question, best_intent)
        
        # Post-process and validate
        final_intent, final_confidence = self._post_process_intent(
            best_intent, confidence, entities, processed_question
        )
        
        return {
            'intent': final_intent,
            'confidence': final_confidence,
            'entities': entities,
            'scores': intent_scores,
            'original_question': question,
            'processed_question': processed_question
        }

    def _preprocess_question(self, question_lower: str) -> str:
        """Preprocess question for better matching"""
        # Remove extra spaces
        processed = re.sub(r'\s+', ' ', question_lower).strip()
        
        # Normalize Thai-English mixed terms
        normalizations = {
            'standard‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î': 'standard ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î',
            'pm‡∏á‡∏≤‡∏ô': 'pm ‡∏á‡∏≤‡∏ô',
            'overhaul compressor': 'overhaul compressor',
        }
        
        for old, new in normalizations.items():
            processed = processed.replace(old.lower(), new.lower())
        
        return processed
    
    def _calculate_intent_scores(self, question: str, previous_intent: Optional[str] = None) -> Dict[str, float]:
        """Calculate scores for all intents"""
        scores = {}
        
        for intent, keywords in self.intent_keywords.items():
            score = 0.0
            
            # Strong keywords (high weight)
            for keyword in keywords.get('strong', []):
                if keyword.lower() in question:
                    score += 10.0
                    # Bonus for exact word match
                    if re.search(rf'\b{re.escape(keyword.lower())}\b', question):
                        score += 2.0
            
            # Medium keywords
            for keyword in keywords.get('medium', []):
                if keyword.lower() in question:
                    score += 5.0
                    if re.search(rf'\b{re.escape(keyword.lower())}\b', question):
                        score += 1.0
            
            # Weak keywords
            for keyword in keywords.get('weak', []):
                if keyword.lower() in question:
                    score += 2.0
            
            # Pattern matching (high value)
            for pattern in keywords.get('patterns', []):
                if re.search(pattern, question, re.IGNORECASE):
                    score += 8.0
            
            # Negative keywords (penalty)
            for neg_keyword in keywords.get('negative', []):
                if neg_keyword.lower() in question:
                    score -= 3.0
            
            # Business domain bonus
            score += self._calculate_domain_bonus(question, intent)
            
            # Previous intent bonus (continuity)
            if previous_intent == intent:
                score += 3.0
            
            scores[intent] = max(0.0, score)
        
        return scores
    
    def _calculate_domain_bonus(self, question: str, intent: str) -> float:
        """Calculate domain-specific bonus"""
        bonus = 0.0
        
        # HVAC equipment terms
        for term in self.business_terms['hvac_equipment']:
            if term.lower() in question:
                if intent in ['spare_parts', 'parts_price', 'repair_history']:
                    bonus += 2.0
                elif intent in ['sales', 'overhaul_report']:
                    bonus += 1.0
        
        # Service type terms
        for term in self.business_terms['service_types']:
            if term.lower() in question:
                if intent in ['work_force', 'work_plan', 'work_summary']:
                    bonus += 2.0
                elif intent in ['sales', 'overhaul_report']:
                    bonus += 1.0
        
        # Brand terms
        for term in self.business_terms['brands']:
            if term.lower() in question:
                if intent in ['customer_history', 'repair_history']:
                    bonus += 3.0
                elif intent in ['spare_parts', 'parts_price']:
                    bonus += 2.0
        
        return bonus
    
    def _get_best_intent_with_confidence(self, scores: Dict[str, float], question: str) -> Tuple[str, float]:
        """Get best intent with calculated confidence"""
        if not scores:
            return 'unknown', 0.0
        
        # Find top 2 intents
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        best_intent, best_score = sorted_scores[0]
        second_score = sorted_scores[1][1] if len(sorted_scores) > 1 else 0.0
        
        # Calculate confidence based on score and separation
        if best_score == 0:
            confidence = 0.0
        else:
            # Base confidence from score
            base_confidence = min(best_score / 30.0, 1.0)  # Normalize to max 30 points
            
            # Separation bonus (how much better than second best)
            if second_score > 0:
                separation = (best_score - second_score) / best_score
                separation_bonus = separation * 0.3  # Up to 30% bonus
            else:
                separation_bonus = 0.3  # Max bonus if only one intent scored
            
            confidence = min(base_confidence + separation_bonus, 1.0)
        
        return best_intent, confidence
    
    def _post_process_intent(self, intent: str, confidence: float, entities: Dict, question: str) -> Tuple[str, float]:
        """Post-process intent based on entities and context"""
        
        # Special rules for pricing detection
        if '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤' in question or 'standard' in question.lower():
            if intent not in ['pricing', 'parts_price']:
                # Force pricing intent if clear pricing indicators
                if 'standard' in question.lower() and '‡∏á‡∏≤‡∏ô' in question:
                    return 'pricing', max(0.8, confidence)
                elif '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà' in question and '‡∏£‡∏≤‡∏Ñ‡∏≤' in question:
                    return 'parts_price', max(0.8, confidence)
        
        # Boost confidence for clear entity matches
        if entities.get('years') and intent in ['sales', 'sales_analysis', 'customer_history']:
            confidence = min(confidence + 0.1, 1.0)
        
        if entities.get('months') and intent in ['work_plan', 'work_summary', 'sales']:
            confidence = min(confidence + 0.1, 1.0)
        
        if entities.get('products') and intent in ['spare_parts', 'parts_price']:
            confidence = min(confidence + 0.15, 1.0)
        
        # Reduce confidence for ambiguous cases
        if confidence < 0.3 and not entities.get('years') and not entities.get('months'):
            confidence = max(confidence - 0.1, 0.0)
        
        return intent, confidence

    # =================================================================
    # ENTITY EXTRACTION (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß)
    # =================================================================
    
    def _extract_entities(self, question: str, intent: str) -> Dict[str, Any]:
        """Enhanced entity extraction"""
        entities = {
            'years': [],
            'months': [],
            'dates': [],
            'products': [],
            'customers': [],
            'amounts': [],
            'job_types': [],
            'brands': []
        }
        
        # Extract years with better patterns
        entities['years'] = self._extract_years(question)
        
        # Extract months
        entities['months'] = self._extract_months(question)
        
        # Extract dates
        entities['dates'] = self._extract_dates(question)
        
        # Extract products/models
        entities['products'] = self._extract_products(question)
        
        # Extract customers/brands
        entities['customers'] = self._extract_customers(question)
        entities['brands'] = self._extract_brands(question)
        
        # Extract job types
        entities['job_types'] = self._extract_job_types(question)
        
        # Extract amounts
        entities['amounts'] = self._extract_amounts(question)
        
        lookback_pattern = r'‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á\s*(\d+)\s*‡∏õ‡∏µ'
        match = re.search(lookback_pattern, question)
        if match:
            years_back = int(match.group(1))
            current_year = 2025  
            # Generate years list
            for i in range(years_back):
                year = current_year - i
                if year >= 2022:  # ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 2022
                    entities['years'].append(year)

        # Clean duplicates and validate
        entities = self._clean_and_validate_entities(entities)
        
        return entities
    
    def _extract_years(self, question: str) -> List[int]:
        """Extract years with improved patterns"""
        years = []
        
        # Thai year patterns (‡∏û.‡∏®.)
        thai_patterns = [
            (r'‡∏õ‡∏µ\s*(\d{4})', lambda m: int(m.group(1))),
            (r'(\d{4})', lambda m: int(m.group(1))),
            (r'25(\d{2})', lambda m: 2000 + int(m.group(1))),  # 2567 -> 67
        ]
        
        for pattern, converter in thai_patterns:
            matches = re.findall(pattern, question)
            for match in matches:
                try:
                    year = int(match) if isinstance(match, str) else converter(re.match(pattern, str(match)))
                    # Convert Thai year to AD
                    if year > 2500:
                        year = year - 543
                    if 2020 <= year <= 2030:
                        years.append(year)
                except:
                    continue
        
        # Handle ranges like 2567-2568
        range_pattern = r'(\d{4})\s*[-‚Äì]\s*(\d{4})'
        range_matches = re.findall(range_pattern, question)
        for start_year, end_year in range_matches:
            try:
                start = int(start_year)
                end = int(end_year)
                if start > 2500:
                    start -= 543
                if end > 2500:
                    end -= 543
                if 2020 <= start <= 2030 and 2020 <= end <= 2030:
                    years.extend(range(start, end + 1))
            except:
                continue
        
        return list(set(years))
    
    def _extract_months(self, question: str) -> List[int]:
        """Extract months from question"""
        months = []
        
        for month_name, month_num in self.month_map.items():
            if month_name.lower() in question.lower():
                months.append(month_num)
        
        # Handle ranges like ‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°-‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô
        range_pattern = r'(\w+)\s*[-‚Äì]\s*(\w+)'
        range_matches = re.findall(range_pattern, question)
        for start_month, end_month in range_matches:
            start_num = self.month_map.get(start_month.lower())
            end_num = self.month_map.get(end_month.lower())
            if start_num and end_num:
                if start_num <= end_num:
                    months.extend(range(start_num, end_num + 1))
                else:
                    # Cross year range (e.g., Nov-Jan)
                    months.extend(range(start_num, 13))
                    months.extend(range(1, end_num + 1))
        
        return list(set(months))
    
    def _extract_dates(self, question: str) -> List[str]:
        """Extract specific dates"""
        dates = []
        
        # Date patterns
        date_patterns = [
            r'\d{1,2}/\d{1,2}/\d{4}',  # DD/MM/YYYY
            r'\d{4}-\d{2}-\d{2}',      # YYYY-MM-DD
            r'\d{1,2}\s+\w+\s+\d{4}'   # DD Month YYYY
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, question)
            dates.extend(matches)
        
        return dates
    
    def _extract_products(self, question: str) -> List[str]:
        """Extract product/model names"""
        products = []
        
        # Common product patterns
        product_patterns = [
            r'EKAC\d+',
            r'RCUG\d+[A-Z]*\d*',
            r'17[A-C]\d{5}[A-Z]?',
            r'EK\s+model\s+(\w+)',
            r'model\s+(\w+)'
        ]
        
        for pattern in product_patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            products.extend(matches)
        
        return list(set(products))
    
    def _extract_customers(self, question: str) -> List[str]:
        """
        Extract customer/company names - COMPREHENSIVE VERSION
        Based on actual database patterns
        """
        customers = []
        question_original = question  # Keep original for case-sensitive
        
        # ========== KNOWN CUSTOMERS FROM DATABASE ==========
        # ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô
        known_customers = {
            # ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ variations
            '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡πå': ['‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡πå‡∏Ø', '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡∏∞', '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡∏∞'],
            
            # English companies
            'CLARION': ['CLARION', 'CLARION ASIA', 'CLARION ASIA (THAILAND)', 'CLARION ASIA ( THAILAND ) CO.,LTD.'],
            'STANLEY': ['STANLEY', 'STANLEY ELECTRIC'],
            'HONDA': ['HONDA', 'HONDA AUTOMOBILE', '‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤', '‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤ ‡∏•‡∏≤‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏±‡∏á'],
            'SADESA': ['SADESA', 'Sadesa', 'SADESA (THAILAND)', '‡∏ã‡∏≤‡πÄ‡∏î‡∏ã‡πà‡∏≤'],
            'AGC': ['AGC', '‡πÄ‡∏≠‡∏à‡∏µ‡∏ã‡∏µ', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏≠‡∏à‡∏µ‡∏ã‡∏µ ‡πÅ‡∏ü‡∏•‡∏ó‡∏Å‡∏•‡∏≤‡∏™'],
            'IRPC': ['IRPC', 'IRPC Public', 'IRPC PUBLIC'],
            'DENSO': ['DENSO', 'DENSO (THAILAND)', '‡πÄ‡∏î‡∏ô‡πÇ‡∏ã‡πà'],
            'SEIKO': ['SEIKO', 'SEIKO INSTRUMENTS'],
            'HITACHI': ['HITACHI', '‡∏Æ‡∏¥‡∏ï‡∏≤‡∏ä‡∏¥', 'Hitachi Astemo', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏Æ‡∏¥‡∏ï‡∏≤‡∏ä‡∏¥ ‡πÅ‡∏≠‡∏™‡πÄ‡∏ï‡πÇ‡∏°'],
            
            # Thai companies
            '‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂': ['‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂ ‡πÅ‡∏°‡∏Å‡πÄ‡∏ô‡∏ï‡∏¥‡∏Ñ', '‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂ ‡πÅ‡∏°‡πá‡∏Ñ‡πÄ‡∏ô‡∏ï‡∏¥‡∏Ñ‡∏™‡πå', '‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂ ‡πÅ‡∏°‡πá‡∏Å‡πÄ‡∏ô‡∏ï‡∏¥‡∏Ñ'],
            '‡∏™‡∏´‡∏Å‡∏•': ['‡∏™‡∏´‡∏Å‡∏•‡∏≠‡∏¥‡∏Ñ‡∏ß‡∏¥‡∏õ‡πÄ‡∏°‡∏ô‡∏ó‡πå', '‡∏™‡∏´‡∏Å‡∏•‡∏≠‡∏¥‡∏Ñ‡∏ß‡∏µ‡∏õ‡πÄ‡∏°‡πâ‡∏ô‡∏ó‡πå', '‡∏™‡∏´‡∏Å‡∏•‡∏≠‡∏¥‡∏Ñ‡∏ß‡∏¥‡∏õ‡πÄ‡∏°‡∏ô‡∏ï‡πå'],
            '‡∏≠‡∏£‡∏∏‡∏ì‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡πå': ['‡∏≠‡∏£‡∏∏‡∏ì‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡πå', '‡∏≠‡∏£‡∏∏‡∏ì‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡πå ‡πÄ‡∏≠‡πá‡∏Å‡∏ã‡πÄ‡∏û‡∏£‡∏™', '‡∏≠‡∏£‡∏∏‡∏ì‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡πå ‡πÄ‡∏≠‡πá‡∏Å‡πÄ‡∏û‡∏£‡∏™'],
            
            # Government
            '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°': ['‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏õ‡∏•‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏õ‡∏•‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°', '‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°'],
            '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤': ['‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏´‡∏•‡∏ß‡∏á', '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ù‡πà‡∏≤‡∏¢‡∏ú‡∏•‡∏¥‡∏ï'],
            
            # Others
            '‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏ì‡∏Å‡∏£': ['‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏ì‡∏Å‡∏£', '‡∏ö‡πâ‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏ì‡∏Å‡∏£', '‡∏Ñ‡∏ì‡∏Å‡∏£ ‡πÄ‡∏ï‡∏ä‡∏≤‡∏´‡∏±‡∏ß‡∏™‡∏¥‡∏á‡∏´‡πå'],
            '‡∏ö‡∏¥‡πÇ‡∏Å‡πâ': ['‡∏ö‡∏µ‡πÇ‡∏Å‡πâ', '‡∏ö‡∏¥‡πÇ‡∏Å‡πâ', '‡∏ö‡∏¥‡πÇ‡∏Å‡πâ ‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô', '‡∏ö‡∏µ‡πÇ‡∏Å‡πâ ‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ'],
        }
        
        # Check for known customers
        question_lower = question.lower()
        for key, variations in known_customers.items():
            for variation in variations:
                if variation.lower() in question_lower:
                    customers.append(variation)
                    # Also add the main key
                    if key not in customers:
                        customers.append(key)
        
        # ========== PATTERN 1: ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö./‡∏ö‡∏à‡∏Å./‡∏ö‡∏°‡∏à. ==========
        thai_company_patterns = [
            # ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó patterns
            r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([^‡∏à]{2,30}(?:‡∏à‡∏≥‡∏Å‡∏±‡∏î|‡∏à‡∏Å\.|‡∏à‡∏Å(?:\s|$)))',
            r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([^\s][^‡∏à]{2,30})',
            
            # ‡∏ö. patterns (short form)
            r'‡∏ö\.\s*([^‡∏à]{2,20}(?:‡∏à‡∏Å\.|‡∏à‡∏≥‡∏Å‡∏±‡∏î|‡∏Ø))',
            r'‡∏ö\.\s*([‡∏Å-‡πôa-zA-Z][^‡∏°]{2,20})',
            
            # ‡∏ö‡∏à‡∏Å. patterns
            r'‡∏ö‡∏à‡∏Å\.\s*([‡∏Å-‡πôa-zA-Z].{2,20})',
            
            # ‡∏ö‡∏°‡∏à. patterns (public company)
            r'‡∏ö‡∏°‡∏à\.\s*([‡∏Å-‡πôa-zA-Z].{2,20})',
            
            # ‡∏´‡∏à‡∏Å. patterns
            r'‡∏´‡∏à‡∏Å\.\s*([‡∏Å-‡πôa-zA-Z].{2,20})',
        ]
        
        for pattern in thai_company_patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            for match in matches:
                # Clean the match
                clean = match.strip()
                # Remove common endings
                clean = re.sub(r'\s*(‡∏°‡∏µ|‡πÑ‡∏î‡πâ|‡πÑ‡∏´‡∏°|‡∏ö‡πâ‡∏≤‡∏á|‡∏Ñ‡∏£‡∏±‡∏ö|‡∏Ñ‡πà‡∏∞|‡πÅ‡∏•‡∏∞|‡∏´‡∏£‡∏∑‡∏≠|‡∏Å‡∏±‡∏ö|‡∏ó‡∏µ‡πà|‡∏Ç‡∏≠‡∏á|‡πÄ‡∏õ‡πá‡∏ô|‡πÄ‡∏ó‡πà‡∏≤|‡∏Å‡∏≤‡∏£|‡∏ã‡∏∑‡πâ‡∏≠|‡∏Ç‡∏≤‡∏¢|‡∏¢‡πâ‡∏≠‡∏ô|‡∏´‡∏•‡∏±‡∏á).*$', '', clean)
                
                if clean and len(clean) > 2:
                    customers.append(clean)
        
        # ========== PATTERN 2: English Company Names ==========
        eng_patterns = [
            # Company with Ltd., Co., Inc.
            r'\b([A-Z][A-Za-z0-9\s\-&.,()]+(?:CO\.|LTD\.|LIMITED|INC\.|CORP\.|Co\.,Ltd\.))',
            
            # All caps names (likely company names)
            r'\b([A-Z]{3,}(?:\s+[A-Z]+)*)\b',
            
            # Mixed case with (THAILAND)
            r'\b([A-Za-z]+(?:\s+[A-Za-z]+)*\s*\(\s*THAILAND\s*\))',
        ]
        
        for pattern in eng_patterns:
            matches = re.findall(pattern, question_original)  # Use original for case
            for match in matches:
                # Filter out common non-company words
                if match.upper() not in ['PM', 'HVAC', 'AHU', 'FCU', 'VRF', 'THE', 'AND', 'OR', 'FOR']:
                    customers.append(match)
        
        # ========== PATTERN 3: ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ ==========
        if '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ' in question:
            clinic_patterns = [
                r'(‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ[‡∏Å-‡πô]*‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ[‡∏Å-‡πô]*)',
                r'(‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ[‡∏Å-‡πô]*‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö[‡∏Å-‡πô]*)',
                r'(‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ[‡∏Å-‡πô]+)',
                r'(‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ\s+[‡∏Å-‡πô]+)',
            ]
            
            for pattern in clinic_patterns:
                matches = re.findall(pattern, question)
                customers.extend(matches)
        
        # ========== PATTERN 4: ‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•/‡∏£‡∏û. ==========
        if '‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•' in question or '‡∏£‡∏û.' in question:
            hospital_patterns = [
                r'(‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•[‡∏Å-‡πôa-zA-Z]+)',
                r'(‡∏£‡∏û\.[‡∏Å-‡πôa-zA-Z]+)',
                r'(‡∏£‡∏û\.\s*[‡∏Å-‡πôa-zA-Z]+)',
            ]
            
            for pattern in hospital_patterns:
                matches = re.findall(pattern, question)
                customers.extend(matches)
        
        # ========== PATTERN 5: ‡∏Ñ‡∏∏‡∏ì + ‡∏ä‡∏∑‡πà‡∏≠ ==========
        if '‡∏Ñ‡∏∏‡∏ì' in question:
            name_pattern = r'‡∏Ñ‡∏∏‡∏ì\s*([‡∏Å-‡πô]+(?:\s+[‡∏Å-‡πô]+)?)'
            matches = re.findall(name_pattern, question)
            for match in matches:
                customers.append(f"‡∏Ñ‡∏∏‡∏ì{match}")
        
        # ========== PATTERN 6: ‡∏™‡∏≥‡∏ô‡∏±‡∏Å/‡∏Å‡∏£‡∏° ==========
        gov_patterns = [
            r'(‡∏™‡∏≥‡∏ô‡∏±‡∏Å[‡∏Å-‡πô]+)',
            r'(‡∏Å‡∏£‡∏°[‡∏Å-‡πô]+)',
            r'(‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤[‡∏Å-‡πô]*)',
        ]
        
        for pattern in gov_patterns:
            matches = re.findall(pattern, question)
            customers.extend(matches)
        
        # ========== CLEAN AND DEDUPLICATE ==========
        cleaned_customers = []
        seen = set()
        
        for customer in customers:
            # Clean
            clean = customer.strip()
            clean = re.sub(r'\s+', ' ', clean)  # Multiple spaces to single
            clean = clean.rstrip(',.;:?!-')  # Remove trailing punctuation
            
            # Must be meaningful
            if len(clean) > 2:
                # Normalize for deduplication
                clean_normalized = clean.lower().replace(' ', '').replace('.', '')
                
                if clean_normalized not in seen:
                    seen.add(clean_normalized)
                    cleaned_customers.append(clean)
                    
                    # Add variations
                    # If "‡∏ö. X" also add "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó X"
                    if clean.startswith('‡∏ö.'):
                        full_form = clean.replace('‡∏ö.', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', 1)
                        if full_form.lower().replace(' ', '') not in seen:
                            cleaned_customers.append(full_form)
                            seen.add(full_form.lower().replace(' ', ''))
                    
                    # If has ‡∏à‡∏Å. also add version with ‡∏à‡∏≥‡∏Å‡∏±‡∏î
                    if '‡∏à‡∏Å.' in clean:
                        full_form = clean.replace('‡∏à‡∏Å.', '‡∏à‡∏≥‡∏Å‡∏±‡∏î')
                        if full_form.lower().replace(' ', '') not in seen:
                            cleaned_customers.append(full_form)
                            seen.add(full_form.lower().replace(' ', ''))
        
        # ========== SPECIAL FALLBACK ==========
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢ ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
        if not cleaned_customers:
            if any(word in question for word in ['‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ö.', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', 'customer']):
                # Try aggressive extraction
                # Extract any Thai phrase between markers
                aggressive_pattern = r'(?:‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó|‡∏ö\.|‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤)\s*([‡∏Å-‡πôa-zA-Z\s]+?)(?:‡∏°‡∏µ|‡πÑ‡∏î‡πâ|‡πÑ‡∏´‡∏°|‡∏Å‡∏≤‡∏£|‡∏ã‡∏∑‡πâ‡∏≠|‡∏Ç‡∏≤‡∏¢|$)'
                match = re.search(aggressive_pattern, question)
                if match:
                    company = match.group(1).strip()
                    if company and len(company) > 2:
                        cleaned_customers.append(company)
        
        # ========== LOGGING ==========
        if cleaned_customers:
            logger.info(f"‚úÖ Found customers: {cleaned_customers}")
        else:
            logger.warning(f"‚ö†Ô∏è No customers found in: {question[:100]}...")
        
        return cleaned_customers



    
    def _extract_brands(self, question: str) -> List[str]:
        """Extract brand names"""
        brands = []
        
        for brand in self.business_terms['brands']:
            if brand.lower() in question.lower():
                brands.append(brand)
        
        return list(set(brands))
    
    def _extract_job_types(self, question: str) -> List[str]:
        """Extract job types"""
        job_types = []
        
        job_keywords = {
            'overhaul': ['overhaul', '‡πÇ‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏Æ‡∏≠‡∏•', '‡∏ã‡πà‡∏≠‡∏°‡πÉ‡∏´‡∏ç‡πà'],
            'replacement': ['replacement', '‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô', '‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà'],
            'PM': ['pm', '‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤', 'maintenance'],
            'service': ['service', '‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏ã‡πà‡∏≠‡∏°']
        }
        
        for job_type, keywords in job_keywords.items():
            for keyword in keywords:
                if keyword.lower() in question.lower():
                    job_types.append(job_type)
                    break
        
        return list(set(job_types))
    
    def _extract_amounts(self, question: str) -> List[str]:
        """Extract amounts/numbers"""
        amounts = []
        
        # Number patterns
        amount_patterns = [
            r'\d{1,3}(?:,\d{3})*\s*‡∏ö‡∏≤‡∏ó',
            r'\$\d{1,3}(?:,\d{3})*',
            r'\d+\s*‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà'
        ]
        
        for pattern in amount_patterns:
            matches = re.findall(pattern, question)
            amounts.extend(matches)
        
        return amounts
    
    def _clean_and_validate_entities(self, entities: Dict[str, Any]) -> Dict[str, Any]:
        """Clean and validate extracted entities"""
        cleaned = {}
        
        for key, value in entities.items():
            if isinstance(value, list):
                # Remove duplicates and empty values
                cleaned_list = list(set([v for v in value if v and str(v).strip()]))
                
                # Sort for consistency
                if key in ['years', 'months']:
                    cleaned_list.sort()
                
                cleaned[key] = cleaned_list
            else:
                cleaned[key] = value
        
        return cleaned

    # =================================================================
    # UTILITY METHODS
    # =================================================================
    
    def get_intent_confidence_report(self, question: str) -> Dict[str, Any]:
        """Get detailed confidence report for debugging"""
        result = self.detect_intent_and_entities(question)
        
        # Sort scores by value
        sorted_scores = sorted(result['scores'].items(), key=lambda x: x[1], reverse=True)
        
        return {
            'question': question,
            'detected_intent': result['intent'],
            'confidence': result['confidence'],
            'entities': result['entities'],
            'all_scores': sorted_scores,
            'top_3_intents': sorted_scores[:3]
        }
    
    def test_intent_accuracy(self, test_cases: List[Tuple[str, str]]) -> Dict[str, Any]:
        """Test intent detection accuracy with known cases"""
        correct = 0
        total = len(test_cases)
        results = []
        
        for question, expected_intent in test_cases:
            result = self.detect_intent_and_entities(question)
            detected = result['intent']
            confidence = result['confidence']
            
            is_correct = detected == expected_intent
            if is_correct:
                correct += 1
            
            results.append({
                'question': question,
                'expected': expected_intent,
                'detected': detected,
                'confidence': confidence,
                'correct': is_correct
            })
        
        return {
            'accuracy': correct / total if total > 0 else 0,
            'correct_count': correct,
            'total_count': total,
            'results': results
        }
  