import os
import re
import json
import asyncio
import time
import logging
import hashlib
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, date, timedelta
from decimal import Decimal
from collections import deque, defaultdict
import aiohttp
import psycopg2
from textwrap import dedent
from psycopg2.extras import RealDictCursor
from collections import Counter, defaultdict
logger = logging.getLogger(__name__)

class ImprovedIntentDetector:
    """
    Enhanced Intent Detector - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    - Keywords ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á
    - Negative keywords ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    - Business domain awareness
    - Better confidence calculation
    """
    
    def __init__(self):
        # =================================================================
        # ENHANCED INTENT KEYWORDS (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å)
        # =================================================================
        self.intent_keywords = {
            'pricing': {
                'strong': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤', 'quotation', 'price', 'cost', 'quote'],
                'medium': ['Standard', '‡∏á‡∏≤‡∏ô', '‡∏™‡∏£‡∏∏‡∏õ', '‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'],  # ‡πÄ‡∏û‡∏¥‡πà‡∏° Standard!
                'weak': ['‡∏ö‡∏≤‡∏ó', '‡πÄ‡∏á‡∏¥‡∏ô', '‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢'],
                'patterns': [
                    r'‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤.*‡∏á‡∏≤‡∏ô',
                    r'‡∏á‡∏≤‡∏ô.*Standard',  # ‡πÄ‡∏û‡∏¥‡πà‡∏° pattern ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                    r'‡∏™‡∏£‡∏∏‡∏õ.*‡∏£‡∏≤‡∏Ñ‡∏≤',
                    r'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£.*‡∏£‡∏≤‡∏Ñ‡∏≤',
                    r'‡∏£‡∏≤‡∏Ñ‡∏≤.*‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏ä‡πà‡∏≤‡∏á', '‡∏ó‡∏µ‡∏°']  # ‡πÄ‡∏≠‡∏≤ '‡∏á‡∏≤‡∏ô' ‡∏≠‡∏≠‡∏Å
            },
            
            'sales': {
                'strong': ['‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', 'revenue', 'sales', 'income', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢'],
                'medium': ['overhaul', 'replacement', 'service', '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏á‡∏≤‡∏ô'],  # ‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤
                'weak': ['‡∏£‡∏ß‡∏°', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', 'total', '‡∏ö‡∏≤‡∏ó'],
                'patterns': [
                    r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏Ç‡∏≤‡∏¢',
                    r'‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ.*‡∏õ‡∏µ',
                    r'‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢.*‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                    r'‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢.*‡∏Ç‡∏≠‡∏á'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏ä‡πà‡∏≤‡∏á', '‡∏ó‡∏µ‡∏°', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']  # ‡πÄ‡∏≠‡∏≤ '‡∏á‡∏≤‡∏ô' ‡∏≠‡∏≠‡∏Å
            },
            
            'sales_analysis': {
                'strong': ['‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå', 'analysis', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô'],
                'medium': ['‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', 'revenue', '‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö'],
                'weak': ['‡∏õ‡∏µ', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡πà‡∏ß‡∏á'],
                'patterns': [
                    r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏Ç‡∏≤‡∏¢',
                    r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ',
                    r'‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö.*‡∏õ‡∏µ'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'overhaul_report': {
                'strong': ['overhaul', '‡πÇ‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏Æ‡∏≠‡∏•', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô'],
                'medium': ['compressor', '‡∏Ñ‡∏≠‡∏°‡πÄ‡∏û‡∏£‡∏™‡πÄ‡∏ã‡∏≠‡∏£‡πå', '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡∏ã‡πà‡∏≠‡∏°'],
                'weak': ['‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏á‡∏≤‡∏ô'],
                'patterns': [
                    r'overhaul.*compressor',
                    r'‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô.*overhaul',
                    r'‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢.*overhaul'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'work_force': {
                'strong': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏ä‡πà‡∏≤‡∏á', 'service_group', '‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô'],
                'medium': ['project', '‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£', 'success', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô'],
                'weak': ['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤'],
                'patterns': [
                    r'‡∏á‡∏≤‡∏ô.*‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                    r'‡∏ó‡∏µ‡∏°.*‡∏á‡∏≤‡∏ô',
                    r'‡∏á‡∏≤‡∏ô.*‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à',
                    r'‡∏á‡∏≤‡∏ô.*‡∏ó‡∏≥'
                ],
                'negative': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤']  # ‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤
            },
            
            'work_plan': {
                'strong': ['‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô', '‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô', 'plan', 'schedule', '‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£'],
                'medium': ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏á‡∏≤‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á', '‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á'],
                'weak': ['‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤', '‡∏ï‡πà‡∏≠‡πÑ‡∏õ'],
                'patterns': [
                    r'‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô.*‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                    r'‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô.*‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà',
                    r'‡∏á‡∏≤‡∏ô.*‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô',
                    r'‡πÅ‡∏ú‡∏ô.*‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á'
                ],
                'negative': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à']
            },
            
            'work_summary': {
                'strong': ['‡∏™‡∏£‡∏∏‡∏õ‡∏á‡∏≤‡∏ô', '‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥', 'summary'],
                'medium': ['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡πà‡∏ß‡∏á', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡πÄ‡∏™‡∏£‡πá‡∏à'],
                'weak': ['‡∏ú‡∏•‡∏á‡∏≤‡∏ô', '‡πÑ‡∏î‡πâ', '‡πÅ‡∏•‡πâ‡∏ß'],
                'patterns': [
                    r'‡∏™‡∏£‡∏∏‡∏õ.*‡∏á‡∏≤‡∏ô',
                    r'‡∏á‡∏≤‡∏ô.*‡∏ó‡∏µ‡πà‡∏ó‡∏≥',
                    r'‡∏á‡∏≤‡∏ô.*‡πÄ‡∏™‡∏£‡πá‡∏à'
                ],
                'negative': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'spare_parts': {
                'strong': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', 'spare', 'part', '‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô'],
                'medium': ['stock', '‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠', '‡∏Ñ‡∏•‡∏±‡∏á', '‡πÄ‡∏Å‡πá‡∏ö'],
                'weak': ['EK', 'model', 'HITACHI', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á'],
                'patterns': [
                    r'‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà.*‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á',
                    r'‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô.*model',
                    r'spare.*part'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'parts_price': {
                'strong': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', 'price'],
                'medium': ['‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á', 'model', '‡∏ó‡∏£‡∏≤‡∏ö', '‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ'],
                'weak': ['‡∏ö‡∏≤‡∏ó', '‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà', 'cost'],
                'patterns': [
                    r'‡∏£‡∏≤‡∏Ñ‡∏≤.*‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà',
                    r'‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà.*‡∏£‡∏≤‡∏Ñ‡∏≤',
                    r'‡∏ó‡∏£‡∏≤‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤.*‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á',
                    r'‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏£‡∏≤‡∏ö.*‡∏£‡∏≤‡∏Ñ‡∏≤'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ']
            },
            
            'inventory_value': {
                'strong': ['‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤', '‡∏Ñ‡∏á‡∏Ñ‡∏•‡∏±‡∏á', 'inventory', 'value'],
                'medium': ['‡∏™‡∏ï‡πá‡∏≠‡∏Å', '‡∏Ñ‡∏•‡∏±‡∏á', '‡πÄ‡∏Å‡πá‡∏ö', '‡∏£‡∏ß‡∏°'],
                'weak': ['‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', 'total'],
                'patterns': [
                    r'‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤.*‡∏Ñ‡∏á‡∏Ñ‡∏•‡∏±‡∏á',
                    r'‡∏™‡∏ï‡πá‡∏≠‡∏Å.*‡∏Ñ‡∏•‡∏±‡∏á'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤']
            },
            
            'customer_history': {
                'strong': ['‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', 'history', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó'],
                'medium': ['‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', 'customer', '‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢', '‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠'],
                'weak': ['‡πÄ‡∏Å‡πà‡∏≤', '‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤', '‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á'],
                'patterns': [
                    r'‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤',
                    r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó.*‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥',
                    r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢',
                    r'‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠.*‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà']
            },
            
            'repair_history': {
                'strong': ['‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', '‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°', '‡∏ã‡πà‡∏≠‡∏°', 'repair'],
                'medium': ['‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏≠‡∏∞‡πÑ‡∏£'],
                'weak': ['‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà', '‡πÄ‡∏Ñ‡∏¢'],
                'patterns': [
                    r'‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥.*‡∏ã‡πà‡∏≠‡∏°',
                    r'‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°.*‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó',
                    r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó.*‡∏ã‡πà‡∏≠‡∏°'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'top_customers': {
                'strong': ['‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', 'Top', '‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö', '‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î'],
                'medium': ['‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡∏´‡∏•‡∏±‡∏Å'],
                'weak': ['5', '10', 'best'],
                'patterns': [
                    r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*Top',
                    r'Top.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤',
                    r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°']
            }
        }

        # =================================================================
        # ENHANCED MONTH MAPPING
        # =================================================================
        self.month_map = {
            # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°
            '‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°': 1, '‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå': 2, '‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°': 3,
            '‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô': 4, '‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°': 5, '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô': 6,
            '‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°': 7, '‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°': 8, '‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô': 9,
            '‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°': 10, '‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô': 11, '‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°': 12,
            
            # ‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠
            '‡∏°.‡∏Ñ.': 1, '‡∏Å.‡∏û.': 2, '‡∏°‡∏µ.‡∏Ñ.': 3,
            '‡πÄ‡∏°.‡∏¢.': 4, '‡∏û.‡∏Ñ.': 5, '‡∏°‡∏¥.‡∏¢.': 6,
            '‡∏Å.‡∏Ñ.': 7, '‡∏™.‡∏Ñ.': 8, '‡∏Å.‡∏¢.': 9,
            '‡∏ï.‡∏Ñ.': 10, '‡∏û.‡∏¢.': 11, '‡∏ò.‡∏Ñ.': 12,
            
            # English
            'january': 1, 'february': 2, 'march': 3,
            'april': 4, 'may': 5, 'june': 6,
            'july': 7, 'august': 8, 'september': 9,
            'october': 10, 'november': 11, 'december': 12,
            
            'jan': 1, 'feb': 2, 'mar': 3,
            'apr': 4, 'jun': 6, 'jul': 7,
            'aug': 8, 'sep': 9, 'oct': 10,
            'nov': 11, 'dec': 12
        }

        # =================================================================
        # BUSINESS-SPECIFIC TERMS
        # =================================================================
        self.business_terms = {
            'hvac_equipment': [
                'chiller', '‡∏Ñ‡∏¥‡∏•‡πÄ‡∏•‡∏≠‡∏£‡πå', '‡πÅ‡∏≠‡∏£‡πå', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏®',
                'compressor', '‡∏Ñ‡∏≠‡∏°‡πÄ‡∏û‡∏£‡∏™‡πÄ‡∏ã‡∏≠‡∏£‡πå', 'AHU', 'FCU'
            ],
            'service_types': [
                'PM', 'maintenance', '‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤', 'overhaul', 
                'replacement', '‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô', '‡∏ã‡πà‡∏≠‡∏°', 'service'
            ],
            'brands': [
                'HITACHI', 'CLARION', 'EK', 'EKAC', 'RCUG', 
                'Sadesa', 'AGC', 'Honda'
            ]
        }

    # =================================================================
    # MAIN DETECTION METHOD
    # =================================================================
    
    def detect_intent_and_entities(self, question: str, 
                                previous_intent: Optional[str] = None) -> Dict[str, Any]:
        """
        FINAL FIX: Enhanced intent detection with proper entity extraction
        Addresses both customer detection and intent mapping issues
        """
        question_lower = question.lower().strip()
        
        # ============================================
        # PRIORITY CHECKS (‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠)
        # ============================================
        
        # 1. CPA Work
        if 'cpa' in question_lower and '‡∏á‡∏≤‡∏ô' in question_lower:
            return {'intent': 'cpa_work', 'confidence': 0.95, 'entities': self._extract_entities(question, 'cpa_work')}
        
        # 2. PM Work
        if 'pm' in question_lower and '‡∏á‡∏≤‡∏ô' in question_lower:
            return {'intent': 'pm_work', 'confidence': 0.90, 'entities': self._extract_entities(question, 'pm_work')}
        
        # ============================================
        # 3. REPAIR HISTORY DETECTION (FIXED)
        # ============================================
        repair_patterns = [
            '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°', '‡∏ã‡πà‡∏≠‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á', 'repair history',
            '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', 'service history', '‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°',
            '‡πÄ‡∏Ñ‡∏¢‡∏ã‡πà‡∏≠‡∏°‡∏≠‡∏∞‡πÑ‡∏£', '‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á'
        ]

        if any(pattern in question_lower for pattern in repair_patterns):
            # ‚úÖ FIX: Extract entities before returning
            entities = self._extract_entities(question, 'customer_repair_history')
            return {
                'intent': 'customer_repair_history', 
                'confidence': 0.95,
                'entities': entities  # ‚úÖ ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ entities!
            }
        
        # ============================================
        # 4. CUSTOMER HISTORY DETECTION (ENHANCED)
        # ============================================
        customer_history_patterns = [
            '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', 'history', '‡πÄ‡∏Ñ‡∏¢.*‡πÉ‡∏ä‡πâ', '‡πÄ‡∏Ñ‡∏¢.*‡∏ã‡∏∑‡πâ‡∏≠', 
            '‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢.*‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á', '‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢', '‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢.*‡∏õ‡∏µ'
        ]
        
        if any(re.search(pattern, question_lower) for pattern in customer_history_patterns):
            entities = self._extract_entities(question, 'customer_history')
            if entities.get('customers'):
                # ‚úÖ Differentiate between repair and sales history
                if any(word in question_lower for word in ['‡∏ã‡πà‡∏≠‡∏°', 'repair', 'service', '‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£']):
                    return {
                        'intent': 'customer_repair_history',
                        'confidence': 0.92,
                        'entities': entities
                    }
                else:
                    return {
                        'intent': 'customer_history', 
                        'confidence': 0.90,
                        'entities': entities
                    }
        
        # ============================================
        # 5. OVERHAUL CONTEXT DETECTION
        # ============================================
        if 'overhaul' in question_lower:
            # Check for SALES context FIRST (higher priority)
            sales_indicators = [
                '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', 'sales', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', 
                'revenue', '‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏ö‡∏≤‡∏ó', 'income'
            ]
            
            # Check for WORK context
            work_indicators = [
                '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô', '‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥', 'work', '‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£', 
                '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î', 'detail', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'date'
            ]
            
            # Priority 1: Sales context wins
            if any(word in question_lower for word in sales_indicators):
                return {
                    'intent': 'overhaul_sales',
                    'confidence': 0.95,
                    'entities': self._extract_entities(question, 'overhaul_sales')
                }
            
            # Priority 2: Explicit work context
            elif any(word in question_lower for word in work_indicators):
                return {
                    'intent': 'work_overhaul',
                    'confidence': 0.90,
                    'entities': self._extract_entities(question, 'work_overhaul')
                }
            
            # Priority 3: Just "‡∏á‡∏≤‡∏ô overhaul" without other context
            elif '‡∏á‡∏≤‡∏ô' in question_lower:
                if '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô' in question_lower:
                    return {
                        'intent': 'overhaul_sales',
                        'confidence': 0.85,
                        'entities': self._extract_entities(question, 'overhaul_sales')
                    }
                else:
                    return {
                        'intent': 'work_overhaul',
                        'confidence': 0.85,
                        'entities': self._extract_entities(question, 'work_overhaul')
                    }
            
            # Priority 4: Just "overhaul" alone
            else:
                return {
                    'intent': 'overhaul_sales',
                    'confidence': 0.80,
                    'entities': self._extract_entities(question, 'overhaul_sales')
                }
        
        # ============================================
        # 6. OTHER SPECIFIC PATTERNS
        # ============================================
        
        # Employee queries
        employee_patterns = ['‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠', '‡∏ó‡∏µ‡∏°‡∏Ç‡∏≠‡∏á', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á']
        if any(pattern in question_lower for pattern in employee_patterns):
            entities = self._extract_entities(question, 'employee_work')
            if entities.get('employees'):
                return {
                    'intent': 'employee_work',
                    'confidence': 0.95,
                    'entities': entities
                }
        
        # ============================================
        # 7. NORMAL PROCESSING (existing logic)
        # ============================================
        
        # Preprocess question
        processed_question = self._preprocess_question(question_lower)
        
        # Calculate intent scores
        intent_scores = self._calculate_intent_scores(processed_question, previous_intent)
        
        # Get best intent with confidence
        best_intent, confidence = self._get_best_intent_with_confidence(intent_scores, processed_question)
        
        # Extract entities
        entities = self._extract_entities(question, best_intent)
        
        # Post-process and validate
        final_intent, final_confidence = self._post_process_intent(
            best_intent, confidence, entities, processed_question
        )
        
        # ============================================
        # 8. FINAL VALIDATION & EMERGENCY FALLBACK
        # ============================================
        
        # Critical validation for customer-related intents
        if final_intent in ['customer_history', 'customer_repair_history']:
            if not entities.get('customers'):
                logger.warning(f"‚ö†Ô∏è {final_intent} intent but no customers found!")
                
                # Try emergency customer extraction
                emergency_customers = self._emergency_customer_extraction(question)
                if emergency_customers:
                    entities['customers'] = emergency_customers
                    logger.info(f"üö® Emergency extraction found: {emergency_customers}")
                else:
                    logger.warning("‚ö†Ô∏è No customers found even with emergency extraction")
                    # Keep the intent but mark as low confidence
                    final_confidence = max(final_confidence - 0.2, 0.3)
        
        return {
            'intent': final_intent,
            'confidence': final_confidence,
            'entities': entities,
            'scores': intent_scores,
            'debug_info': {
                'original_question': question,
                'processed_question': processed_question,
                'customers_found': len(entities.get('customers', [])),
                'emergency_extraction_used': bool(entities.get('customers')) and final_intent in ['customer_history', 'customer_repair_history']
            }
        }

    def _emergency_customer_extraction(self, question: str) -> List[str]:
        """
        Emergency customer extraction for failed cases
        Enhanced patterns for Thai/foreign mixed names
        """
        logger.info("üö® Starting emergency customer extraction...")
        
        emergency_patterns = [
            # Pattern 1: ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó + ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏ô‡∏ñ‡∏∂‡∏á action words
            r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([^,\.\?!]{3,30}?)(?:\s+(?:‡∏°‡∏µ|‡πÄ‡∏Ñ‡∏¢|‡∏ã‡πà‡∏≠‡∏°|‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥|‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£)|$)',
            
            # Pattern 2: ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ + ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏ô‡∏ñ‡∏∂‡∏á action words  
            r'‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ\s*([^,\.\?!]{3,30}?)(?:\s+(?:‡∏°‡∏µ|‡πÄ‡∏Ñ‡∏¢|‡∏ã‡πà‡∏≠‡∏°|‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥|‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£)|$)',
            
            # Pattern 3: Thai/foreign mixed names directly
            r'(‡πÅ‡∏ã‡∏î\s*‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤|‡∏ã‡∏≤‡∏î\s*‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤|‡πÅ‡∏ã‡∏î‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤|‡∏ã‡∏≤‡∏î‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤)',
            
            # Pattern 4: Common business names
            r'(STANLEY|CLARION|HONDA|AGC|HITACHI)\s+(?:‡∏°‡∏µ|‡πÄ‡∏Ñ‡∏¢|‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥)',
            
            # Pattern 5: Generic company extraction before action
            r'([A-Z]{2,}(?:\s+[A-Z][a-z]+)*)\s+(?:‡∏°‡∏µ|‡πÄ‡∏Ñ‡∏¢|‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥)',
        ]
        
        customers = []
        for i, pattern in enumerate(emergency_patterns):
            matches = re.findall(pattern, question, re.IGNORECASE)
            for match in matches:
                clean = match.strip()
                if len(clean) > 2:
                    customers.append(clean)
                    logger.info(f"üö® Emergency pattern {i+1} found: '{clean}'")
        
        # Add hardcoded known problematic names
        known_problematic = ['‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤']
        for name in known_problematic:
            if name in question.lower():
                customers.append(name)
                logger.info(f"üö® Known problematic name found: '{name}'")
        
        return list(set(customers))

    def _preprocess_question(self, question_lower: str) -> str:
        """Preprocess question for better matching"""
        # Remove extra spaces
        processed = re.sub(r'\s+', ' ', question_lower).strip()
        
        # Normalize Thai-English mixed terms
        normalizations = {
            'standard‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î': 'standard ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î',
            'pm‡∏á‡∏≤‡∏ô': 'pm ‡∏á‡∏≤‡∏ô',
            'overhaul compressor': 'overhaul compressor',
        }
        
        for old, new in normalizations.items():
            processed = processed.replace(old.lower(), new.lower())
        
        return processed
    
    def _calculate_intent_scores(self, question: str, previous_intent: Optional[str] = None) -> Dict[str, float]:
        """Calculate scores for all intents"""
        scores = {}
        
        for intent, keywords in self.intent_keywords.items():
            score = 0.0
            
            # Strong keywords (high weight)
            for keyword in keywords.get('strong', []):
                if keyword.lower() in question:
                    score += 10.0
                    # Bonus for exact word match
                    if re.search(rf'\b{re.escape(keyword.lower())}\b', question):
                        score += 2.0
            
            # Medium keywords
            for keyword in keywords.get('medium', []):
                if keyword.lower() in question:
                    score += 5.0
                    if re.search(rf'\b{re.escape(keyword.lower())}\b', question):
                        score += 1.0
            
            # Weak keywords
            for keyword in keywords.get('weak', []):
                if keyword.lower() in question:
                    score += 2.0
            
            # Pattern matching (high value)
            for pattern in keywords.get('patterns', []):
                if re.search(pattern, question, re.IGNORECASE):
                    score += 8.0
            
            # Negative keywords (penalty)
            for neg_keyword in keywords.get('negative', []):
                if neg_keyword.lower() in question:
                    score -= 3.0
            
            # Business domain bonus
            score += self._calculate_domain_bonus(question, intent)
            
            # Previous intent bonus (continuity)
            if previous_intent == intent:
                score += 3.0
            
            scores[intent] = max(0.0, score)
        
        return scores
    
    def _calculate_domain_bonus(self, question: str, intent: str) -> float:
        """Calculate domain-specific bonus"""
        bonus = 0.0
        
        # HVAC equipment terms
        for term in self.business_terms['hvac_equipment']:
            if term.lower() in question:
                if intent in ['spare_parts', 'parts_price', 'repair_history']:
                    bonus += 2.0
                elif intent in ['sales', 'overhaul_report']:
                    bonus += 1.0
        
        # Service type terms
        for term in self.business_terms['service_types']:
            if term.lower() in question:
                if intent in ['work_force', 'work_plan', 'work_summary']:
                    bonus += 2.0
                elif intent in ['sales', 'overhaul_report']:
                    bonus += 1.0
        
        # Brand terms
        for term in self.business_terms['brands']:
            if term.lower() in question:
                if intent in ['customer_history', 'repair_history']:
                    bonus += 3.0
                elif intent in ['spare_parts', 'parts_price']:
                    bonus += 2.0
        
        return bonus
    
    def _get_best_intent_with_confidence(self, scores: Dict[str, float], question: str) -> Tuple[str, float]:
        """Get best intent with calculated confidence"""
        if not scores:
            return 'unknown', 0.0
        
        # Find top 2 intents
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        best_intent, best_score = sorted_scores[0]
        second_score = sorted_scores[1][1] if len(sorted_scores) > 1 else 0.0
        
        # Calculate confidence based on score and separation
        if best_score == 0:
            confidence = 0.0
        else:
            # Base confidence from score
            base_confidence = min(best_score / 30.0, 1.0)  # Normalize to max 30 points
            
            # Separation bonus (how much better than second best)
            if second_score > 0:
                separation = (best_score - second_score) / best_score
                separation_bonus = separation * 0.3  # Up to 30% bonus
            else:
                separation_bonus = 0.3  # Max bonus if only one intent scored
            
            confidence = min(base_confidence + separation_bonus, 1.0)
        
        return best_intent, confidence
    
    def _post_process_intent(self, intent: str, confidence: float, entities: Dict, question: str) -> Tuple[str, float]:
        """Post-process intent based on entities and context"""
        
        # Special rules for pricing detection
        if '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤' in question or 'standard' in question.lower():
            if intent not in ['pricing', 'parts_price']:
                # Force pricing intent if clear pricing indicators
                if 'standard' in question.lower() and '‡∏á‡∏≤‡∏ô' in question:
                    return 'pricing', max(0.8, confidence)
                elif '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà' in question and '‡∏£‡∏≤‡∏Ñ‡∏≤' in question:
                    return 'parts_price', max(0.8, confidence)
        
        # Boost confidence for clear entity matches
        if entities.get('years') and intent in ['sales', 'sales_analysis', 'customer_history']:
            confidence = min(confidence + 0.1, 1.0)
        
        if entities.get('months') and intent in ['work_plan', 'work_summary', 'sales']:
            confidence = min(confidence + 0.1, 1.0)
        
        if entities.get('products') and intent in ['spare_parts', 'parts_price']:
            confidence = min(confidence + 0.15, 1.0)
        
        # Reduce confidence for ambiguous cases
        if confidence < 0.3 and not entities.get('years') and not entities.get('months'):
            confidence = max(confidence - 0.1, 0.0)
        
        return intent, confidence

    # =================================================================
    # ENTITY EXTRACTION (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß)
    # =================================================================
    
    def _extract_entities(self, question: str, intent: str) -> Dict[str, Any]:
        """Enhanced entity extraction"""
        entities = {
            'years': [],
            'months': [],
            'dates': [],
            'products': [],
            'customers': [],
            'amounts': [],
            'job_types': [],
            'brands': []
        }
        
        # Extract years with better patterns
        entities['years'] = self._extract_years(question)
        
        # Extract months
        entities['months'] = self._extract_months(question)
        
        # Extract dates
        entities['dates'] = self._extract_dates(question)
        
        # Extract products/models
        entities['products'] = self._extract_products(question)
        
        # Extract customers/brands
        entities['customers'] = self._extract_customers(question)
        entities['brands'] = self._extract_brands(question)
        
        # Extract job types
        entities['job_types'] = self._extract_job_types(question)
        
        # Extract amounts
        entities['amounts'] = self._extract_amounts(question)
        
        lookback_pattern = r'‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á\s*(\d+)\s*‡∏õ‡∏µ'
        match = re.search(lookback_pattern, question)
        if match:
            years_back = int(match.group(1))
            current_year = 2025  
            # Generate years list
            for i in range(years_back):
                year = current_year - i
                if year >= 2022:  # ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 2022
                    entities['years'].append(year)

        # Clean duplicates and validate
        entities = self._clean_and_validate_entities(entities)
        
        return entities
    
    def _extract_years(self, question: str) -> List[int]:
        """Extract years with improved patterns"""
        years = []
        
        # Thai year patterns (‡∏û.‡∏®.)
        thai_patterns = [
            (r'‡∏õ‡∏µ\s*(\d{4})', lambda m: int(m.group(1))),
            (r'(\d{4})', lambda m: int(m.group(1))),
            (r'25(\d{2})', lambda m: 2000 + int(m.group(1))),  # 2567 -> 67
        ]
        
        for pattern, converter in thai_patterns:
            matches = re.findall(pattern, question)
            for match in matches:
                try:
                    year = int(match) if isinstance(match, str) else converter(re.match(pattern, str(match)))
                    # Convert Thai year to AD
                    if year > 2500:
                        year = year - 543
                    if 2020 <= year <= 2030:
                        years.append(year)
                except:
                    continue
        
        # Handle ranges like 2567-2568
        range_pattern = r'(\d{4})\s*[-‚Äì]\s*(\d{4})'
        range_matches = re.findall(range_pattern, question)
        for start_year, end_year in range_matches:
            try:
                start = int(start_year)
                end = int(end_year)
                if start > 2500:
                    start -= 543
                if end > 2500:
                    end -= 543
                if 2020 <= start <= 2030 and 2020 <= end <= 2030:
                    years.extend(range(start, end + 1))
            except:
                continue
        
        return list(set(years))
    
    def _extract_months(self, question: str) -> List[int]:
        """Extract months from question"""
        months = []
        
        for month_name, month_num in self.month_map.items():
            if month_name.lower() in question.lower():
                months.append(month_num)
        
        # Handle ranges like ‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°-‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô
        range_pattern = r'(\w+)\s*[-‚Äì]\s*(\w+)'
        range_matches = re.findall(range_pattern, question)
        for start_month, end_month in range_matches:
            start_num = self.month_map.get(start_month.lower())
            end_num = self.month_map.get(end_month.lower())
            if start_num and end_num:
                if start_num <= end_num:
                    months.extend(range(start_num, end_num + 1))
                else:
                    # Cross year range (e.g., Nov-Jan)
                    months.extend(range(start_num, 13))
                    months.extend(range(1, end_num + 1))
        
        return list(set(months))
    
    def _extract_dates(self, question: str) -> List[str]:
        """Extract specific dates"""
        dates = []
        
        # Date patterns
        date_patterns = [
            r'\d{1,2}/\d{1,2}/\d{4}',  # DD/MM/YYYY
            r'\d{4}-\d{2}-\d{2}',      # YYYY-MM-DD
            r'\d{1,2}\s+\w+\s+\d{4}'   # DD Month YYYY
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, question)
            dates.extend(matches)
        
        return dates
    
    def _extract_products(self, question: str) -> List[str]:
        """Extract product/model names"""
        products = []
        
        # Common product patterns
        product_patterns = [
            r'EKAC\d+',
            r'RCUG\d+[A-Z]*\d*',
            r'17[A-C]\d{5}[A-Z]?',
            r'EK\s+model\s+(\w+)',
            r'model\s+(\w+)'
        ]
        
        for pattern in product_patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            products.extend(matches)
        
        return list(set(products))
    
    def _extract_customers(self, question: str) -> List[str]:
        """
        FIXED: Enhanced customer extraction with robust Thai/foreign name handling
        Addresses the specific issue: "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤" not being detected
        """
        
        # Early exit for non-customer queries
        non_customer_patterns = [
            r'‡∏°‡∏µ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏Å‡∏µ‡πà', r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏Å‡∏µ‡πà', r'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤',
            r'‡∏ã‡∏∑‡πâ‡∏≠.*‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á', r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', r'‡∏£‡∏ß‡∏°.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤'
        ]
        
        question_lower = question.lower()
        logger.info(f"üîç Question lower: '{question_lower}'")
        
        for pattern in non_customer_patterns:
            if re.search(pattern, question_lower):
                logger.info(f"üö´ EARLY EXIT: Non-customer query detected")
                return []
        
        customers = []
        question_original = question
        
        # ========================================
        # PHASE 1: ENHANCED KNOWN CUSTOMERS DATABASE
        # ========================================
        
        known_customers = {
            # ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ variations
            '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡πå': [
                '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡πå‡∏Ø', '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡∏∞', 
                '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡∏∞'
            ],
            
            # ‚úÖ FIX: Add missing Thai/foreign companies
            '‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤': ['‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤', '‡πÅ‡∏ã‡∏î‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤', 'SADEKURODA'],
            
            # English companies
            'CLARION': ['CLARION', 'CLARION ASIA', 'CLARION ASIA (THAILAND)'],
            'STANLEY': ['STANLEY', 'STANLEY ELECTRIC'],
            'HONDA': ['HONDA', 'HONDA AUTOMOBILE', '‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤'],
            'SADESA': ['SADESA', 'Sadesa', 'SADESA (THAILAND)', '‡∏ã‡∏≤‡πÄ‡∏î‡∏ã‡πà‡∏≤'],
            'AGC': ['AGC', '‡πÄ‡∏≠‡∏à‡∏µ‡∏ã‡∏µ', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏≠‡∏à‡∏µ‡∏ã‡∏µ ‡πÅ‡∏ü‡∏•‡∏ó‡∏Å‡∏•‡∏≤‡∏™'],
            
            # Thai companies with foreign names
            '‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂': ['‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂ ‡πÅ‡∏°‡∏Å‡πÄ‡∏ô‡∏ï‡∏¥‡∏Ñ', '‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂ ‡πÅ‡∏°‡πá‡∏Ñ‡πÄ‡∏ô‡∏ï‡∏¥‡∏Ñ‡∏™‡πå'],
            '‡∏™‡∏´‡∏Å‡∏•': ['‡∏™‡∏´‡∏Å‡∏•‡∏≠‡∏¥‡∏Ñ‡∏ß‡∏¥‡∏õ‡πÄ‡∏°‡∏ô‡∏ó‡πå', '‡∏™‡∏´‡∏Å‡∏•‡∏≠‡∏¥‡∏Ñ‡∏ß‡∏µ‡∏õ‡πÄ‡∏°‡πâ‡∏ô‡∏ó‡πå'],
            
            # Government entities
            '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°': ['‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏õ‡∏•‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°', '‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°'],
            '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤': ['‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏´‡∏•‡∏ß‡∏á', '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ù‡πà‡∏≤‡∏¢‡∏ú‡∏•‡∏¥‡∏ï'],
        }
        
        # Check for known customers first (with fuzzy matching)
        for key, variations in known_customers.items():
            for variation in variations:
                # ‚úÖ Case-insensitive and space-tolerant matching
                variation_clean = re.sub(r'\s+', ' ', variation.lower().strip())
                question_clean = re.sub(r'\s+', ' ', question_lower.strip())
                
                if variation_clean in question_clean:
                    customers.extend([variation, key])
                    logger.info(f"‚úÖ Found known customer via fuzzy match: {variation} -> {key}")
        
        if customers:
            logger.info(f"‚úÖ Found known customers: {list(set(customers))}")
            return list(set(customers))
        
        # ========================================
        # PHASE 2: ENHANCED PATTERN EXTRACTION
        # ========================================
        
        # Only extract if there are clear customer indicators
        customer_indicators = [
            '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ö.', '‡∏ö‡∏à‡∏Å.', '‡∏ö‡∏°‡∏à.', '‡∏´‡∏à‡∏Å.',
            '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ', '‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•', '‡∏£‡∏û.', 
            '‡∏Ñ‡∏∏‡∏ì', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å', '‡∏Å‡∏£‡∏°',
            '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠'
        ]
        
        has_customer_indicator = any(indicator in question for indicator in customer_indicators)
        
        if not has_customer_indicator:
            logger.info(f"üö´ No customer indicators found")
            return []
        
        # ‚úÖ ENHANCED Pattern 1: ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (with flexible suffix handling)
        thai_company_patterns = [
            # Original patterns with required suffixes
            r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([^\s].{2,30}?)(?:\s+(?:‡∏à‡∏≥‡∏Å‡∏±‡∏î|‡∏à‡∏Å\.|‡∏Ø))',
            r'‡∏ö\.\s*([^\s].{2,20}?)(?:\s+(?:‡∏à‡∏Å\.|‡∏à‡∏≥‡∏Å‡∏±‡∏î|‡∏Ø))',
            
            # ‚úÖ NEW: Flexible patterns without required suffixes
            r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([^\s][^,\.\?!]{2,30}?)(?=\s+(?:‡∏°‡∏µ|‡πÑ‡∏î‡πâ|‡πÄ‡∏Ñ‡∏¢|‡∏ã‡πà‡∏≠‡∏°|‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥|‡∏Å‡∏µ‡πà)|$)',
            r'‡∏ö\.\s*([^\s][^,\.\?!]{2,20}?)(?=\s+(?:‡∏°‡∏µ|‡πÑ‡∏î‡πâ|‡πÄ‡∏Ñ‡∏¢|‡∏ã‡πà‡∏≠‡∏°|‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥|‡∏Å‡∏µ‡πà)|$)',
            
            # ‚úÖ NEW: Thai/foreign mixed names
            r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([‡∏Å-‡πôA-Za-z\s]{3,25})(?=\s|$)',
        ]
        
        for pattern in thai_company_patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            for match in matches:
                clean = match.strip()
                # ‚úÖ Better validation (allow Thai/foreign mixed names)
                if (len(clean) > 2 and 
                    not any(word in clean.lower() for word in ['‡∏°‡∏µ', '‡πÑ‡∏î‡πâ', '‡πÑ‡∏´‡∏°', '‡∏Å‡∏µ‡πà', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', '‡∏Å‡∏≤‡∏£', '‡∏ã‡∏∑‡πâ‡∏≠', '‡∏Ç‡∏≤‡∏¢'])):
                    customers.append(clean)
                    logger.info(f"‚úÖ Extracted via Thai company pattern: '{clean}'")
        
        # ‚úÖ ENHANCED Pattern 2: English Company Names (more flexible)
        if re.search(r'\b[A-Z]{2,}', question):  # At least 2 caps
            eng_patterns = [
                r'\b([A-Z][A-Za-z0-9\s\-&.,()]{2,30}(?:CO\.|LTD\.|LIMITED|INC\.|CORP\.))',
                r'\b([A-Z]{2,}(?:\s+[A-Z]+)*)\b',  # Multiple caps
                r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)\b'  # Title case
            ]
            
            for pattern in eng_patterns:
                matches = re.findall(pattern, question_original)
                for match in matches:
                    clean = match.strip()
                    # Enhanced exclusion list
                    if (clean.upper() not in ['HVAC', 'FCU', 'AHU', 'VRF', 'THE', 'AND', 'PM', 'OR', 'BUT'] and
                        len(clean) > 2):
                        customers.append(clean)
                        logger.info(f"‚úÖ Extracted via English pattern: '{clean}'")
        
        # ‚úÖ ENHANCED Pattern 3: Thai foreign names (without company prefix)
        if '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó' in question and not customers:
            # More aggressive extraction for Thai/foreign mixed names
            thai_foreign_patterns = [
                r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([‡∏Å-‡πôA-Za-z\s]+)',  # Allow mixed Thai/English
                r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([^\s][^‡πÜ]{2,25})',  # General pattern
            ]
            
            for pattern in thai_foreign_patterns:
                matches = re.findall(pattern, question)
                for match in matches:
                    clean_name = match.strip()
                    # Enhanced filtering
                    excluded_words = ['‡∏°‡∏µ', '‡πÑ‡∏î‡πâ', '‡πÑ‡∏´‡∏°', '‡∏Å‡∏≤‡∏£', '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', '‡∏ã‡πà‡∏≠‡∏°', '‡∏ö‡πâ‡∏≤‡∏á', '‡∏≠‡∏∞‡πÑ‡∏£', '‡πÄ‡∏ó‡πà‡∏≤', '‡πÑ‡∏´‡∏£‡πà']
                    if (clean_name not in excluded_words and 
                        len(clean_name) > 3 and
                        not any(word in clean_name.lower() for word in excluded_words)):
                        customers.append(clean_name)
                        logger.info(f"‚úÖ Extracted Thai foreign name: '{clean_name}'")
        
        # ‚úÖ ENHANCED Emergency fallbacks for common problematic names
        emergency_fallbacks = [
            ('‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤', ['‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤', '‡πÅ‡∏ã‡∏î‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤']),
            ('‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂', ['‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂', 'SHINETSU']),
            ('‡∏™‡∏´‡∏Å‡∏•', ['‡∏™‡∏´‡∏Å‡∏•', '‡∏™‡∏´‡∏Å‡∏•‡∏≠‡∏¥‡∏Ñ‡∏ß‡∏¥‡∏õ']),
        ]
        
        if not customers:
            for canonical_name, variations in emergency_fallbacks:
                for variant in variations:
                    if variant in question:
                        customers.append(canonical_name)
                        logger.info(f"‚úÖ Emergency fallback: {variant} -> {canonical_name}")
                        break
        
        # ‚úÖ Pattern 4: ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ (enhanced)
        if '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ' in question and not customers:
            clinic_patterns = [
                r'(‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ[‡∏Å-‡πô\s]+?)(?=\s+(?:‡∏°‡∏µ|‡πÄ‡∏Ñ‡∏¢|‡πÑ‡∏î‡πâ|‡∏ã‡πà‡∏≠‡∏°|‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥|‡∏Å‡∏µ‡πà)|$)',
                r'(‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ[^‡∏Å‡∏µ‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢]{3,30}?)(?=\s|$)',
            ]
            
            for pattern in clinic_patterns:
                matches = re.findall(pattern, question)
                for match in matches:
                    clinic_name = match.strip()
                    if len(clinic_name) > 6:  # ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ = 6 chars + name
                        customers.append(clinic_name)
                        logger.info(f"‚úÖ Extracted clinic: '{clinic_name}'")
        
        # ========================================
        # PHASE 3: CLEAN AND VALIDATE
        # ========================================
        
        cleaned_customers = []
        for customer in customers:
            clean = customer.strip()
            clean = re.sub(r'\s+', ' ', clean)  # Normalize spaces
            clean = clean.rstrip(',.;:?!-')
            
            # Final validation
            if (len(clean) > 2 and 
                not any(bad_word in clean.lower() for bad_word in 
                    ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', '‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢', '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå', '‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£'])):
                cleaned_customers.append(clean)
        
        # Remove duplicates while preserving order
        cleaned_customers = list(dict.fromkeys(cleaned_customers))
        
        # ========================================
        # LOGGING AND RETURN
        # ========================================
        
        if cleaned_customers:
            logger.info(f"‚úÖ Final extracted customers: {cleaned_customers}")
        else:
            logger.info(f"‚ÑπÔ∏è No customers extracted - will be treated as general query")
        
        return cleaned_customers


    
    def _extract_brands(self, question: str) -> List[str]:
        """Extract brand names"""
        brands = []
        
        for brand in self.business_terms['brands']:
            if brand.lower() in question.lower():
                brands.append(brand)
        
        return list(set(brands))
    
    def _extract_job_types(self, question: str) -> List[str]:
        """Extract job types"""
        job_types = []
        
        job_keywords = {
            'overhaul': ['overhaul', '‡πÇ‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏Æ‡∏≠‡∏•', '‡∏ã‡πà‡∏≠‡∏°‡πÉ‡∏´‡∏ç‡πà'],
            'replacement': ['replacement', '‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô', '‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà'],
            'PM': ['pm', '‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤', 'maintenance'],
            'service': ['service', '‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏ã‡πà‡∏≠‡∏°']
        }
        
        for job_type, keywords in job_keywords.items():
            for keyword in keywords:
                if keyword.lower() in question.lower():
                    job_types.append(job_type)
                    break
        
        return list(set(job_types))
    
    def _extract_amounts(self, question: str) -> List[str]:
        """Extract amounts/numbers"""
        amounts = []
        
        # Number patterns
        amount_patterns = [
            r'\d{1,3}(?:,\d{3})*\s*‡∏ö‡∏≤‡∏ó',
            r'\$\d{1,3}(?:,\d{3})*',
            r'\d+\s*‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà'
        ]
        
        for pattern in amount_patterns:
            matches = re.findall(pattern, question)
            amounts.extend(matches)
        
        return amounts
    
    def _clean_and_validate_entities(self, entities: Dict[str, Any]) -> Dict[str, Any]:
        """Clean and validate extracted entities"""
        cleaned = {}
        
        for key, value in entities.items():
            if isinstance(value, list):
                # Remove duplicates and empty values
                cleaned_list = list(set([v for v in value if v and str(v).strip()]))
                
                # Sort for consistency
                if key in ['years', 'months']:
                    cleaned_list.sort()
                
                cleaned[key] = cleaned_list
            else:
                cleaned[key] = value
        
        return cleaned

    # =================================================================
    # UTILITY METHODS
    # =================================================================
    
    def get_intent_confidence_report(self, question: str) -> Dict[str, Any]:
        """Get detailed confidence report for debugging"""
        result = self.detect_intent_and_entities(question)
        
        # Sort scores by value
        sorted_scores = sorted(result['scores'].items(), key=lambda x: x[1], reverse=True)
        
        return {
            'question': question,
            'detected_intent': result['intent'],
            'confidence': result['confidence'],
            'entities': result['entities'],
            'all_scores': sorted_scores,
            'top_3_intents': sorted_scores[:3]
        }
    
    def test_intent_accuracy(self, test_cases: List[Tuple[str, str]]) -> Dict[str, Any]:
        """Test intent detection accuracy with known cases"""
        correct = 0
        total = len(test_cases)
        results = []
        
        for question, expected_intent in test_cases:
            result = self.detect_intent_and_entities(question)
            detected = result['intent']
            confidence = result['confidence']
            
            is_correct = detected == expected_intent
            if is_correct:
                correct += 1
            
            results.append({
                'question': question,
                'expected': expected_intent,
                'detected': detected,
                'confidence': confidence,
                'correct': is_correct
            })
        
        return {
            'accuracy': correct / total if total > 0 else 0,
            'correct_count': correct,
            'total_count': total,
            'results': results
        }
  