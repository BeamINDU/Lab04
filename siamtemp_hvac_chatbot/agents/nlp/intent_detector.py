import os
import re
import json
import asyncio
import time
import logging
import hashlib
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, date, timedelta
from decimal import Decimal
from collections import deque, defaultdict
import aiohttp
import psycopg2
from textwrap import dedent
from psycopg2.extras import RealDictCursor
from collections import Counter, defaultdict
logger = logging.getLogger(__name__)

class ImprovedIntentDetector:
    """
    Enhanced Intent Detector - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    - Keywords ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á
    - Negative keywords ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    - Business domain awareness
    - Better confidence calculation
    """
    
    def __init__(self):
        # =================================================================
        # ENHANCED INTENT KEYWORDS (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å)
        # =================================================================
        self.intent_keywords = {
            'pricing': {
                'strong': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤', 'quotation', 'price', 'cost', 'quote'],
                'medium': ['Standard', '‡∏á‡∏≤‡∏ô', '‡∏™‡∏£‡∏∏‡∏õ', '‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'],  # ‡πÄ‡∏û‡∏¥‡πà‡∏° Standard!
                'weak': ['‡∏ö‡∏≤‡∏ó', '‡πÄ‡∏á‡∏¥‡∏ô', '‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢'],
                'patterns': [
                    r'‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤.*‡∏á‡∏≤‡∏ô',
                    r'‡∏á‡∏≤‡∏ô.*Standard',  # ‡πÄ‡∏û‡∏¥‡πà‡∏° pattern ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                    r'‡∏™‡∏£‡∏∏‡∏õ.*‡∏£‡∏≤‡∏Ñ‡∏≤',
                    r'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£.*‡∏£‡∏≤‡∏Ñ‡∏≤',
                    r'‡∏£‡∏≤‡∏Ñ‡∏≤.*‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏ä‡πà‡∏≤‡∏á', '‡∏ó‡∏µ‡∏°']  # ‡πÄ‡∏≠‡∏≤ '‡∏á‡∏≤‡∏ô' ‡∏≠‡∏≠‡∏Å
            },
            
            'sales': {
                'strong': ['‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', 'revenue', 'sales', 'income', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢'],
                'medium': ['overhaul', 'replacement', 'service', '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏á‡∏≤‡∏ô'],  # ‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤
                'weak': ['‡∏£‡∏ß‡∏°', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', 'total', '‡∏ö‡∏≤‡∏ó'],
                'patterns': [
                    r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏Ç‡∏≤‡∏¢',
                    r'‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ.*‡∏õ‡∏µ',
                    r'‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢.*‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                    r'‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢.*‡∏Ç‡∏≠‡∏á'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏ä‡πà‡∏≤‡∏á', '‡∏ó‡∏µ‡∏°', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']  # ‡πÄ‡∏≠‡∏≤ '‡∏á‡∏≤‡∏ô' ‡∏≠‡∏≠‡∏Å
            },
            
            'sales_analysis': {
                'strong': ['‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå', 'analysis', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô'],
                'medium': ['‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', 'revenue', '‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö'],
                'weak': ['‡∏õ‡∏µ', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡πà‡∏ß‡∏á'],
                'patterns': [
                    r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏Ç‡∏≤‡∏¢',
                    r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ',
                    r'‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö.*‡∏õ‡∏µ'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'overhaul_report': {
                'strong': ['overhaul', '‡πÇ‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏Æ‡∏≠‡∏•', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô'],
                'medium': ['compressor', '‡∏Ñ‡∏≠‡∏°‡πÄ‡∏û‡∏£‡∏™‡πÄ‡∏ã‡∏≠‡∏£‡πå', '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡∏ã‡πà‡∏≠‡∏°'],
                'weak': ['‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏á‡∏≤‡∏ô'],
                'patterns': [
                    r'overhaul.*compressor',
                    r'‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô.*overhaul',
                    r'‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢.*overhaul'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'work_force': {
                'strong': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏ä‡πà‡∏≤‡∏á', 'service_group', '‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô'],
                'medium': ['project', '‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£', 'success', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô'],
                'weak': ['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤'],
                'patterns': [
                    r'‡∏á‡∏≤‡∏ô.*‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                    r'‡∏ó‡∏µ‡∏°.*‡∏á‡∏≤‡∏ô',
                    r'‡∏á‡∏≤‡∏ô.*‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à',
                    r'‡∏á‡∏≤‡∏ô.*‡∏ó‡∏≥'
                ],
                'negative': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤']  # ‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤
            },
            
            'work_plan': {
                'strong': ['‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô', '‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô', 'plan', 'schedule', '‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£'],
                'medium': ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏á‡∏≤‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á', '‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á'],
                'weak': ['‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤', '‡∏ï‡πà‡∏≠‡πÑ‡∏õ'],
                'patterns': [
                    r'‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô.*‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                    r'‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô.*‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà',
                    r'‡∏á‡∏≤‡∏ô.*‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô',
                    r'‡πÅ‡∏ú‡∏ô.*‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á'
                ],
                'negative': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à']
            },
            
            'work_summary': {
                'strong': ['‡∏™‡∏£‡∏∏‡∏õ‡∏á‡∏≤‡∏ô', '‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥', 'summary'],
                'medium': ['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡πà‡∏ß‡∏á', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡πÄ‡∏™‡∏£‡πá‡∏à'],
                'weak': ['‡∏ú‡∏•‡∏á‡∏≤‡∏ô', '‡πÑ‡∏î‡πâ', '‡πÅ‡∏•‡πâ‡∏ß'],
                'patterns': [
                    r'‡∏™‡∏£‡∏∏‡∏õ.*‡∏á‡∏≤‡∏ô',
                    r'‡∏á‡∏≤‡∏ô.*‡∏ó‡∏µ‡πà‡∏ó‡∏≥',
                    r'‡∏á‡∏≤‡∏ô.*‡πÄ‡∏™‡∏£‡πá‡∏à'
                ],
                'negative': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'spare_parts': {
                'strong': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', 'spare', 'part', '‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô'],
                'medium': ['stock', '‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠', '‡∏Ñ‡∏•‡∏±‡∏á', '‡πÄ‡∏Å‡πá‡∏ö'],
                'weak': ['EK', 'model', 'HITACHI', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á'],
                'patterns': [
                    r'‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà.*‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á',
                    r'‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô.*model',
                    r'spare.*part'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'parts_price': {
                'strong': ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', 'price'],
                'medium': ['‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á', 'model', '‡∏ó‡∏£‡∏≤‡∏ö', '‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ'],
                'weak': ['‡∏ö‡∏≤‡∏ó', '‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà', 'cost'],
                'patterns': [
                    r'‡∏£‡∏≤‡∏Ñ‡∏≤.*‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà',
                    r'‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà.*‡∏£‡∏≤‡∏Ñ‡∏≤',
                    r'‡∏ó‡∏£‡∏≤‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤.*‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á',
                    r'‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏£‡∏≤‡∏ö.*‡∏£‡∏≤‡∏Ñ‡∏≤'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ']
            },
            
            'inventory_value': {
                'strong': ['‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤', '‡∏Ñ‡∏á‡∏Ñ‡∏•‡∏±‡∏á', 'inventory', 'value'],
                'medium': ['‡∏™‡∏ï‡πá‡∏≠‡∏Å', '‡∏Ñ‡∏•‡∏±‡∏á', '‡πÄ‡∏Å‡πá‡∏ö', '‡∏£‡∏ß‡∏°'],
                'weak': ['‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', 'total'],
                'patterns': [
                    r'‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤.*‡∏Ñ‡∏á‡∏Ñ‡∏•‡∏±‡∏á',
                    r'‡∏™‡∏ï‡πá‡∏≠‡∏Å.*‡∏Ñ‡∏•‡∏±‡∏á'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤']
            },
            
            'customer_history': {
                'strong': ['‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', 'history', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó'],
                'medium': ['‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', 'customer', '‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢', '‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠'],
                'weak': ['‡πÄ‡∏Å‡πà‡∏≤', '‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤', '‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á'],
                'patterns': [
                    r'‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤',
                    r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó.*‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥',
                    r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢',
                    r'‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠.*‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á'
                ],
                'negative': ['‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°', '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà']
            },
            
            'repair_history': {
                'strong': ['‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', '‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°', '‡∏ã‡πà‡∏≠‡∏°', 'repair'],
                'medium': ['‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏≠‡∏∞‡πÑ‡∏£'],
                'weak': ['‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà', '‡πÄ‡∏Ñ‡∏¢'],
                'patterns': [
                    r'‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥.*‡∏ã‡πà‡∏≠‡∏°',
                    r'‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°.*‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó',
                    r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó.*‡∏ã‡πà‡∏≠‡∏°'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô']
            },
            
            'top_customers': {
                'strong': ['‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', 'Top', '‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö', '‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î'],
                'medium': ['‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡∏´‡∏•‡∏±‡∏Å'],
                'weak': ['5', '10', 'best'],
                'patterns': [
                    r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*Top',
                    r'Top.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤',
                    r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î'
                ],
                'negative': ['‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà', '‡∏á‡∏≤‡∏ô', '‡∏ó‡∏µ‡∏°']
            }
        }

        # =================================================================
        # ENHANCED MONTH MAPPING
        # =================================================================
        self.month_map = {
            # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°
            '‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°': 1, '‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå': 2, '‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°': 3,
            '‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô': 4, '‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°': 5, '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô': 6,
            '‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°': 7, '‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°': 8, '‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô': 9,
            '‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°': 10, '‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô': 11, '‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°': 12,
            
            # ‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠
            '‡∏°.‡∏Ñ.': 1, '‡∏Å.‡∏û.': 2, '‡∏°‡∏µ.‡∏Ñ.': 3,
            '‡πÄ‡∏°.‡∏¢.': 4, '‡∏û.‡∏Ñ.': 5, '‡∏°‡∏¥.‡∏¢.': 6,
            '‡∏Å.‡∏Ñ.': 7, '‡∏™.‡∏Ñ.': 8, '‡∏Å.‡∏¢.': 9,
            '‡∏ï.‡∏Ñ.': 10, '‡∏û.‡∏¢.': 11, '‡∏ò.‡∏Ñ.': 12,
            
            # English
            'january': 1, 'february': 2, 'march': 3,
            'april': 4, 'may': 5, 'june': 6,
            'july': 7, 'august': 8, 'september': 9,
            'october': 10, 'november': 11, 'december': 12,
            
            'jan': 1, 'feb': 2, 'mar': 3,
            'apr': 4, 'jun': 6, 'jul': 7,
            'aug': 8, 'sep': 9, 'oct': 10,
            'nov': 11, 'dec': 12
        }

        # =================================================================
        # BUSINESS-SPECIFIC TERMS
        # =================================================================
        self.business_terms = {
            'hvac_equipment': [
                'chiller', '‡∏Ñ‡∏¥‡∏•‡πÄ‡∏•‡∏≠‡∏£‡πå', '‡πÅ‡∏≠‡∏£‡πå', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏®',
                'compressor', '‡∏Ñ‡∏≠‡∏°‡πÄ‡∏û‡∏£‡∏™‡πÄ‡∏ã‡∏≠‡∏£‡πå', 'AHU', 'FCU'
            ],
            'service_types': [
                'PM', 'maintenance', '‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤', 'overhaul', 
                'replacement', '‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô', '‡∏ã‡πà‡∏≠‡∏°', 'service'
            ],
            'brands': [
                'HITACHI', 'CLARION', 'EK', 'EKAC', 'RCUG', 
                'Sadesa', 'AGC', 'Honda'
            ]
        }

    # =================================================================
    # MAIN DETECTION METHOD
    # =================================================================
    
    def detect_intent_and_entities(self, question: str, 
                                previous_intent: Optional[str] = None) -> Dict[str, Any]:
        """
        Enhanced intent detection with better accuracy
        FIXED: Handle overhaul with sales/work context properly
        """
        question_lower = question.lower().strip()
        
        # ============================================
        # PRIORITY CHECKS (‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠)
        # ============================================
        
        # 1. CPA Work
        if 'cpa' in question_lower and '‡∏á‡∏≤‡∏ô' in question_lower:
            return {'intent': 'cpa_work', 'confidence': 0.95, 'entities': self._extract_entities(question, 'cpa_work')}
        
        # 2. PM Work
        if 'pm' in question_lower and '‡∏á‡∏≤‡∏ô' in question_lower:
            return {'intent': 'pm_work', 'confidence': 0.90, 'entities': self._extract_entities(question, 'pm_work')}
        
        # ============================================
        # üîß FIX: OVERHAUL CONTEXT DETECTION
        # ============================================
        if 'overhaul' in question_lower:
            # Check for SALES context FIRST (higher priority)
            sales_indicators = [
                '‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', 'sales', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ', 
                'revenue', '‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏ö‡∏≤‡∏ó', 'income'
            ]
            
            # Check for WORK context
            work_indicators = [
                '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô', '‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥', 'work', '‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£', 
                '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î', 'detail', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'date'
            ]
            
            # Priority 1: Sales context wins
            if any(word in question_lower for word in sales_indicators):
                return {
                    'intent': 'overhaul_sales',  # ‚úÖ Sales intent
                    'confidence': 0.95,
                    'entities': self._extract_entities(question, 'overhaul_sales')
                }
            
            # Priority 2: Explicit work context
            elif any(word in question_lower for word in work_indicators):
                return {
                    'intent': 'work_overhaul',  # Work intent
                    'confidence': 0.90,
                    'entities': self._extract_entities(question, 'work_overhaul')
                }
            
            # Priority 3: Just "‡∏á‡∏≤‡∏ô overhaul" without other context
            elif '‡∏á‡∏≤‡∏ô' in question_lower:
                # Default to work unless has "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô"
                if '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô' in question_lower:
                    return {
                        'intent': 'overhaul_sales',  # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô = sales report
                        'confidence': 0.85,
                        'entities': self._extract_entities(question, 'overhaul_sales')
                    }
                else:
                    return {
                        'intent': 'work_overhaul',
                        'confidence': 0.85,
                        'entities': self._extract_entities(question, 'work_overhaul')
                    }
            
            # Priority 4: Just "overhaul" alone - check other context
            else:
                # Default to sales for standalone "overhaul"
                return {
                    'intent': 'overhaul_sales',
                    'confidence': 0.80,
                    'entities': self._extract_entities(question, 'overhaul_sales')
                }
        
        # ============================================
        # OTHER SPECIFIC PATTERNS
        # ============================================
        
        # Employee queries
        employee_patterns = ['‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠', '‡∏ó‡∏µ‡∏°‡∏Ç‡∏≠‡∏á', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á']
        if any(pattern in question_lower for pattern in employee_patterns):
            entities = self._extract_entities(question, 'employee_work')
            if entities.get('employees'):
                return {
                    'intent': 'employee_work',
                    'confidence': 0.95,
                    'entities': entities
                }
        repair_patterns = [
            '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°', '‡∏ã‡πà‡∏≠‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á', 'repair history',
            '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', 'service history'
        ]

        if any(pattern in question_lower for pattern in repair_patterns):
            return {'intent': 'customer_repair_history', 'confidence': 0.95}
        
        # Customer history
        if any(word in question_lower for word in ['‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', 'history', '‡πÄ‡∏Ñ‡∏¢', '‡∏ã‡πà‡∏≠‡∏°']):
            entities = self._extract_entities(question, 'customer_history')
            if entities.get('customers'):
                return {
                    'intent': 'customer_history',
                    'confidence': 0.90,
                    'entities': entities
                }
        
        # ============================================
        # NORMAL PROCESSING (existing logic)
        # ============================================
        
        # Preprocess question
        processed_question = self._preprocess_question(question_lower)
        
        # Calculate intent scores
        intent_scores = self._calculate_intent_scores(processed_question, previous_intent)
        
        # Get best intent with confidence
        best_intent, confidence = self._get_best_intent_with_confidence(intent_scores, processed_question)
        
        # Extract entities
        entities = self._extract_entities(question, best_intent)
        
        # Post-process and validate
        final_intent, final_confidence = self._post_process_intent(
            best_intent, confidence, entities, processed_question
        )
        
        return {
            'intent': final_intent,
            'confidence': final_confidence,
            'entities': entities,
            'scores': intent_scores,
            'original_question': question,
            'processed_question': processed_question
        }

    def _preprocess_question(self, question_lower: str) -> str:
        """Preprocess question for better matching"""
        # Remove extra spaces
        processed = re.sub(r'\s+', ' ', question_lower).strip()
        
        # Normalize Thai-English mixed terms
        normalizations = {
            'standard‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î': 'standard ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î',
            'pm‡∏á‡∏≤‡∏ô': 'pm ‡∏á‡∏≤‡∏ô',
            'overhaul compressor': 'overhaul compressor',
        }
        
        for old, new in normalizations.items():
            processed = processed.replace(old.lower(), new.lower())
        
        return processed
    
    def _calculate_intent_scores(self, question: str, previous_intent: Optional[str] = None) -> Dict[str, float]:
        """Calculate scores for all intents"""
        scores = {}
        
        for intent, keywords in self.intent_keywords.items():
            score = 0.0
            
            # Strong keywords (high weight)
            for keyword in keywords.get('strong', []):
                if keyword.lower() in question:
                    score += 10.0
                    # Bonus for exact word match
                    if re.search(rf'\b{re.escape(keyword.lower())}\b', question):
                        score += 2.0
            
            # Medium keywords
            for keyword in keywords.get('medium', []):
                if keyword.lower() in question:
                    score += 5.0
                    if re.search(rf'\b{re.escape(keyword.lower())}\b', question):
                        score += 1.0
            
            # Weak keywords
            for keyword in keywords.get('weak', []):
                if keyword.lower() in question:
                    score += 2.0
            
            # Pattern matching (high value)
            for pattern in keywords.get('patterns', []):
                if re.search(pattern, question, re.IGNORECASE):
                    score += 8.0
            
            # Negative keywords (penalty)
            for neg_keyword in keywords.get('negative', []):
                if neg_keyword.lower() in question:
                    score -= 3.0
            
            # Business domain bonus
            score += self._calculate_domain_bonus(question, intent)
            
            # Previous intent bonus (continuity)
            if previous_intent == intent:
                score += 3.0
            
            scores[intent] = max(0.0, score)
        
        return scores
    
    def _calculate_domain_bonus(self, question: str, intent: str) -> float:
        """Calculate domain-specific bonus"""
        bonus = 0.0
        
        # HVAC equipment terms
        for term in self.business_terms['hvac_equipment']:
            if term.lower() in question:
                if intent in ['spare_parts', 'parts_price', 'repair_history']:
                    bonus += 2.0
                elif intent in ['sales', 'overhaul_report']:
                    bonus += 1.0
        
        # Service type terms
        for term in self.business_terms['service_types']:
            if term.lower() in question:
                if intent in ['work_force', 'work_plan', 'work_summary']:
                    bonus += 2.0
                elif intent in ['sales', 'overhaul_report']:
                    bonus += 1.0
        
        # Brand terms
        for term in self.business_terms['brands']:
            if term.lower() in question:
                if intent in ['customer_history', 'repair_history']:
                    bonus += 3.0
                elif intent in ['spare_parts', 'parts_price']:
                    bonus += 2.0
        
        return bonus
    
    def _get_best_intent_with_confidence(self, scores: Dict[str, float], question: str) -> Tuple[str, float]:
        """Get best intent with calculated confidence"""
        if not scores:
            return 'unknown', 0.0
        
        # Find top 2 intents
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        best_intent, best_score = sorted_scores[0]
        second_score = sorted_scores[1][1] if len(sorted_scores) > 1 else 0.0
        
        # Calculate confidence based on score and separation
        if best_score == 0:
            confidence = 0.0
        else:
            # Base confidence from score
            base_confidence = min(best_score / 30.0, 1.0)  # Normalize to max 30 points
            
            # Separation bonus (how much better than second best)
            if second_score > 0:
                separation = (best_score - second_score) / best_score
                separation_bonus = separation * 0.3  # Up to 30% bonus
            else:
                separation_bonus = 0.3  # Max bonus if only one intent scored
            
            confidence = min(base_confidence + separation_bonus, 1.0)
        
        return best_intent, confidence
    
    def _post_process_intent(self, intent: str, confidence: float, entities: Dict, question: str) -> Tuple[str, float]:
        """Post-process intent based on entities and context"""
        
        # Special rules for pricing detection
        if '‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤' in question or 'standard' in question.lower():
            if intent not in ['pricing', 'parts_price']:
                # Force pricing intent if clear pricing indicators
                if 'standard' in question.lower() and '‡∏á‡∏≤‡∏ô' in question:
                    return 'pricing', max(0.8, confidence)
                elif '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà' in question and '‡∏£‡∏≤‡∏Ñ‡∏≤' in question:
                    return 'parts_price', max(0.8, confidence)
        
        # Boost confidence for clear entity matches
        if entities.get('years') and intent in ['sales', 'sales_analysis', 'customer_history']:
            confidence = min(confidence + 0.1, 1.0)
        
        if entities.get('months') and intent in ['work_plan', 'work_summary', 'sales']:
            confidence = min(confidence + 0.1, 1.0)
        
        if entities.get('products') and intent in ['spare_parts', 'parts_price']:
            confidence = min(confidence + 0.15, 1.0)
        
        # Reduce confidence for ambiguous cases
        if confidence < 0.3 and not entities.get('years') and not entities.get('months'):
            confidence = max(confidence - 0.1, 0.0)
        
        return intent, confidence

    # =================================================================
    # ENTITY EXTRACTION (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß)
    # =================================================================
    
    def _extract_entities(self, question: str, intent: str) -> Dict[str, Any]:
        """Enhanced entity extraction"""
        entities = {
            'years': [],
            'months': [],
            'dates': [],
            'products': [],
            'customers': [],
            'amounts': [],
            'job_types': [],
            'brands': []
        }
        
        # Extract years with better patterns
        entities['years'] = self._extract_years(question)
        
        # Extract months
        entities['months'] = self._extract_months(question)
        
        # Extract dates
        entities['dates'] = self._extract_dates(question)
        
        # Extract products/models
        entities['products'] = self._extract_products(question)
        
        # Extract customers/brands
        entities['customers'] = self._extract_customers(question)
        entities['brands'] = self._extract_brands(question)
        
        # Extract job types
        entities['job_types'] = self._extract_job_types(question)
        
        # Extract amounts
        entities['amounts'] = self._extract_amounts(question)
        
        lookback_pattern = r'‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á\s*(\d+)\s*‡∏õ‡∏µ'
        match = re.search(lookback_pattern, question)
        if match:
            years_back = int(match.group(1))
            current_year = 2025  
            # Generate years list
            for i in range(years_back):
                year = current_year - i
                if year >= 2022:  # ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 2022
                    entities['years'].append(year)

        # Clean duplicates and validate
        entities = self._clean_and_validate_entities(entities)
        
        return entities
    
    def _extract_years(self, question: str) -> List[int]:
        """Extract years with improved patterns"""
        years = []
        
        # Thai year patterns (‡∏û.‡∏®.)
        thai_patterns = [
            (r'‡∏õ‡∏µ\s*(\d{4})', lambda m: int(m.group(1))),
            (r'(\d{4})', lambda m: int(m.group(1))),
            (r'25(\d{2})', lambda m: 2000 + int(m.group(1))),  # 2567 -> 67
        ]
        
        for pattern, converter in thai_patterns:
            matches = re.findall(pattern, question)
            for match in matches:
                try:
                    year = int(match) if isinstance(match, str) else converter(re.match(pattern, str(match)))
                    # Convert Thai year to AD
                    if year > 2500:
                        year = year - 543
                    if 2020 <= year <= 2030:
                        years.append(year)
                except:
                    continue
        
        # Handle ranges like 2567-2568
        range_pattern = r'(\d{4})\s*[-‚Äì]\s*(\d{4})'
        range_matches = re.findall(range_pattern, question)
        for start_year, end_year in range_matches:
            try:
                start = int(start_year)
                end = int(end_year)
                if start > 2500:
                    start -= 543
                if end > 2500:
                    end -= 543
                if 2020 <= start <= 2030 and 2020 <= end <= 2030:
                    years.extend(range(start, end + 1))
            except:
                continue
        
        return list(set(years))
    
    def _extract_months(self, question: str) -> List[int]:
        """Extract months from question"""
        months = []
        
        for month_name, month_num in self.month_map.items():
            if month_name.lower() in question.lower():
                months.append(month_num)
        
        # Handle ranges like ‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°-‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô
        range_pattern = r'(\w+)\s*[-‚Äì]\s*(\w+)'
        range_matches = re.findall(range_pattern, question)
        for start_month, end_month in range_matches:
            start_num = self.month_map.get(start_month.lower())
            end_num = self.month_map.get(end_month.lower())
            if start_num and end_num:
                if start_num <= end_num:
                    months.extend(range(start_num, end_num + 1))
                else:
                    # Cross year range (e.g., Nov-Jan)
                    months.extend(range(start_num, 13))
                    months.extend(range(1, end_num + 1))
        
        return list(set(months))
    
    def _extract_dates(self, question: str) -> List[str]:
        """Extract specific dates"""
        dates = []
        
        # Date patterns
        date_patterns = [
            r'\d{1,2}/\d{1,2}/\d{4}',  # DD/MM/YYYY
            r'\d{4}-\d{2}-\d{2}',      # YYYY-MM-DD
            r'\d{1,2}\s+\w+\s+\d{4}'   # DD Month YYYY
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, question)
            dates.extend(matches)
        
        return dates
    
    def _extract_products(self, question: str) -> List[str]:
        """Extract product/model names"""
        products = []
        
        # Common product patterns
        product_patterns = [
            r'EKAC\d+',
            r'RCUG\d+[A-Z]*\d*',
            r'17[A-C]\d{5}[A-Z]?',
            r'EK\s+model\s+(\w+)',
            r'model\s+(\w+)'
        ]
        
        for pattern in product_patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            products.extend(matches)
        
        return list(set(products))
    
    def _extract_customers(self, question: str) -> List[str]:
        non_customer_patterns = [
            # Count/frequency questions
            r'‡∏°‡∏µ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏Å‡∏µ‡πà', r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏Å‡∏µ‡πà', r'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤',
            r'‡∏ã‡∏∑‡πâ‡∏≠.*‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á', r'‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≤.*‡∏Å‡∏µ‡πà', r'‡∏°‡∏µ.*‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢.*‡∏Å‡∏µ‡πà',
            
            # Summary/aggregate questions  
            r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', r'‡∏£‡∏ß‡∏°.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', r'total.*customer',
            r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î', r'top.*customer', r'‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤',
            
            # General questions about customers
            r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡πÉ‡∏´‡∏°‡πà', r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡πÄ‡∏Å‡πà‡∏≤', r'‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤.*‡∏°‡∏≤‡∏Å',
            r'customer.*new', r'customer.*old', r'customer.*most',
            
            # Analysis questions
            r'‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', r'‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', r'‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô.*‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤'
        ]
        
        question_lower = question.lower()
        logger.info(f"üîç Question lower: '{question_lower}'")
        # Check if this is a non-customer query
        for pattern in non_customer_patterns:
            if re.search(pattern, question_lower):
                logger.info(f"üö´ EARLY EXIT: Non-customer query detected - pattern: {pattern}")
                return []
        
        # Additional word-based filtering
        non_customer_words = [
            '‡∏Å‡∏µ‡πà‡∏£‡∏≤‡∏¢', '‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô', '‡∏Å‡∏µ‡πà‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á',
            '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', '‡∏£‡∏ß‡∏°', 'total', 'count',
            '‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î', '‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î',
            '‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö', '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô'
        ]
        
        if any(word in question_lower for word in non_customer_words):
            logger.info(f"üö´ EARLY EXIT: Found non-customer indicator words")
            return []
        
        # ========================================
        # PROCEED WITH NORMAL CUSTOMER EXTRACTION
        # ========================================
        
        customers = []
        question_original = question  # Keep original for case-sensitive
        
        # ========== KNOWN CUSTOMERS FROM DATABASE ==========
        known_customers = {
            # ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ variations
            '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡πå': ['‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡πå‡∏Ø', '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡∏∞', '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏®‡∏¥‡∏•‡∏õ‡∏∞'],
            
            # English companies
            'CLARION': ['CLARION', 'CLARION ASIA', 'CLARION ASIA (THAILAND)', 'CLARION ASIA ( THAILAND ) CO.,LTD.'],
            'STANLEY': ['STANLEY', 'STANLEY ELECTRIC'],
            'HONDA': ['HONDA', 'HONDA AUTOMOBILE', '‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤', '‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤ ‡∏•‡∏≤‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏±‡∏á'],
            'SADESA': ['SADESA', 'Sadesa', 'SADESA (THAILAND)', '‡∏ã‡∏≤‡πÄ‡∏î‡∏ã‡πà‡∏≤'],
            'AGC': ['AGC', '‡πÄ‡∏≠‡∏à‡∏µ‡∏ã‡∏µ', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏≠‡∏à‡∏µ‡∏ã‡∏µ ‡πÅ‡∏ü‡∏•‡∏ó‡∏Å‡∏•‡∏≤‡∏™'],
            'IRPC': ['IRPC', 'IRPC Public', 'IRPC PUBLIC'],
            'DENSO': ['DENSO', 'DENSO (THAILAND)', '‡πÄ‡∏î‡∏ô‡πÇ‡∏ã‡πà'],
            'SEIKO': ['SEIKO', 'SEIKO INSTRUMENTS'],
            'HITACHI': ['HITACHI', '‡∏Æ‡∏¥‡∏ï‡∏≤‡∏ä‡∏¥', 'Hitachi Astemo', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏Æ‡∏¥‡∏ï‡∏≤‡∏ä‡∏¥ ‡πÅ‡∏≠‡∏™‡πÄ‡∏ï‡πÇ‡∏°'],
            
            # Thai companies
            '‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂': ['‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂ ‡πÅ‡∏°‡∏Å‡πÄ‡∏ô‡∏ï‡∏¥‡∏Ñ', '‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂ ‡πÅ‡∏°‡πá‡∏Ñ‡πÄ‡∏ô‡∏ï‡∏¥‡∏Ñ‡∏™‡πå', '‡∏ä‡∏¥‡∏ô‡∏≠‡∏¥‡∏ó‡∏ã‡∏∂ ‡πÅ‡∏°‡πá‡∏Å‡πÄ‡∏ô‡∏ï‡∏¥‡∏Ñ'],
            '‡∏™‡∏´‡∏Å‡∏•': ['‡∏™‡∏´‡∏Å‡∏•‡∏≠‡∏¥‡∏Ñ‡∏ß‡∏¥‡∏õ‡πÄ‡∏°‡∏ô‡∏ó‡πå', '‡∏™‡∏´‡∏Å‡∏•‡∏≠‡∏¥‡∏Ñ‡∏ß‡∏µ‡∏õ‡πÄ‡∏°‡πâ‡∏ô‡∏ó‡πå', '‡∏™‡∏´‡∏Å‡∏•‡∏≠‡∏¥‡∏Ñ‡∏ß‡∏¥‡∏õ‡πÄ‡∏°‡∏ô‡∏ï‡πå'],
            
            # Government
            '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°': ['‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏õ‡∏•‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏õ‡∏•‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°', '‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°'],
            '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤': ['‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏´‡∏•‡∏ß‡∏á', '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ù‡πà‡∏≤‡∏¢‡∏ú‡∏•‡∏¥‡∏ï'],
        }
        
        # Check for known customers first
        for key, variations in known_customers.items():
            for variation in variations:
                if variation.lower() in question_lower:
                    customers.append(variation)
                    if key not in customers:
                        customers.append(key)
        
        # If we found known customers, return early
        if customers:
            logger.info(f"‚úÖ Found known customers: {customers}")
            return customers
        
        # ========================================
        # SPECIFIC CUSTOMER NAME EXTRACTION
        # ========================================
        
        # Only extract if there are clear customer indicators
        customer_indicators = [
            '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ö.', '‡∏ö‡∏à‡∏Å.', '‡∏ö‡∏°‡∏à.', '‡∏´‡∏à‡∏Å.',
            '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ', '‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•', '‡∏£‡∏û.', 
            '‡∏Ñ‡∏∏‡∏ì', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å', '‡∏Å‡∏£‡∏°',
            '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤', '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠'
        ]
        
        has_customer_indicator = any(indicator in question for indicator in customer_indicators)
        
        if not has_customer_indicator:
            logger.info(f"üö´ No customer indicators found")
            return []
        
        # ========== PATTERN EXTRACTION ==========
        
        # Pattern 1: ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö./‡∏ö‡∏à‡∏Å./‡∏ö‡∏°‡∏à.
        thai_company_patterns = [
            r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([^\s].{2,30}?)(?:\s+(?:‡∏à‡∏≥‡∏Å‡∏±‡∏î|‡∏à‡∏Å\.|‡∏Ø)|$)',
            r'‡∏ö\.\s*([^\s].{2,20}?)(?:\s+(?:‡∏à‡∏Å\.|‡∏à‡∏≥‡∏Å‡∏±‡∏î|‡∏Ø)|$)',
            r'‡∏ö‡∏à‡∏Å\.\s*([^\s].{2,20})',
            r'‡∏ö‡∏°‡∏à\.\s*([^\s].{2,20})',
        ]
        
        for pattern in thai_company_patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            for match in matches:
                clean = match.strip()
                # Exclude non-company words
                if not any(word in clean.lower() for word in ['‡∏°‡∏µ', '‡πÑ‡∏î‡πâ', '‡πÑ‡∏´‡∏°', '‡∏Å‡∏µ‡πà', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', '‡∏Å‡∏≤‡∏£']):
                    if len(clean) > 2:
                        customers.append(clean)
        
        # Pattern 2: English Company Names (more restrictive)
        if re.search(r'\b[A-Z]{3,}', question):  # Only if there are caps
            eng_patterns = [
                r'\b([A-Z][A-Za-z0-9\s\-&.,()]+(?:CO\.|LTD\.|LIMITED|INC\.|CORP\.))',
                r'\b([A-Z]{3,}(?:\s+[A-Z]+)*)\b',
            ]
            
            for pattern in eng_patterns:
                matches = re.findall(pattern, question_original)
                for match in matches:
                    if match.upper() not in ['HVAC', 'FCU', 'AHU', 'VRF', 'THE', 'AND', 'PM']:
                        customers.append(match)


        if '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó' in question and not customers:
            # More aggressive extraction for Thai foreign names
            pattern = r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s*([‡∏Å-‡πô]+(?:\s+[‡∏Å-‡πô]+)*)'
            matches = re.findall(pattern, question)
            for match in matches:
                clean_name = match.strip()
                # Filter out common words
                if clean_name not in ['‡∏°‡∏µ', '‡πÑ‡∏î‡πâ', '‡πÑ‡∏´‡∏°', '‡∏Å‡∏≤‡∏£', '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', '‡∏ã‡πà‡∏≠‡∏°', '‡∏ö‡πâ‡∏≤‡∏á']:
                    if len(clean_name) > 3:
                        customers.append(clean_name)
                        logger.info(f"‚úÖ Extracted Thai foreign name: {clean_name}")
        
        # ‚úÖ EMERGENCY fallback for "‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤" type
        if not customers and '‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤' in question:
            customers.append('‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤')
            logger.info("‚úÖ Emergency fallback: ‡πÅ‡∏ã‡∏î ‡∏Ñ‡∏π‡πÇ‡∏£‡∏î‡∏≤")

        # Pattern 3: ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ (specific extraction)
        if '‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ' in question:
            clinic_match = re.search(r'(‡∏Ñ‡∏•‡∏µ‡∏ô‡∏¥‡∏Ñ[‡∏Å-‡πô]*[^‡∏Å‡∏µ‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢]*)', question)
            if clinic_match:
                clinic_name = clinic_match.group(1).strip()
                # Make sure it's not part of a counting question
                if not any(word in clinic_name for word in ['‡∏Å‡∏µ‡πà', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', '‡∏Å‡∏≤‡∏£', '‡∏ã‡∏∑‡πâ‡∏≠', '‡∏Ç‡∏≤‡∏¢']):
                    customers.append(clinic_name)
        
        # ========== CLEAN AND VALIDATE ==========
        cleaned_customers = []
        
        for customer in customers:
            clean = customer.strip()
            clean = re.sub(r'\s+', ' ', clean)
            clean = clean.rstrip(',.;:?!-')
            
            # Final validation
            if (len(clean) > 2 and 
                not any(bad_word in clean.lower() for bad_word in 
                    ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á', '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', '‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢', '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå'])):
                cleaned_customers.append(clean)
        
        # Remove duplicates
        cleaned_customers = list(dict.fromkeys(cleaned_customers))
        
        # ========================================
        # NO AGGRESSIVE FALLBACK - SAFER APPROACH
        # ========================================
        
        # Log results
        if cleaned_customers:
            logger.info(f"‚úÖ Found customers: {cleaned_customers}")
        else:
            logger.info(f"‚ÑπÔ∏è No specific customers found - treating as general query")
        
        return cleaned_customers



    
    def _extract_brands(self, question: str) -> List[str]:
        """Extract brand names"""
        brands = []
        
        for brand in self.business_terms['brands']:
            if brand.lower() in question.lower():
                brands.append(brand)
        
        return list(set(brands))
    
    def _extract_job_types(self, question: str) -> List[str]:
        """Extract job types"""
        job_types = []
        
        job_keywords = {
            'overhaul': ['overhaul', '‡πÇ‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏Æ‡∏≠‡∏•', '‡∏ã‡πà‡∏≠‡∏°‡πÉ‡∏´‡∏ç‡πà'],
            'replacement': ['replacement', '‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô', '‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà'],
            'PM': ['pm', '‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤', 'maintenance'],
            'service': ['service', '‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏ã‡πà‡∏≠‡∏°']
        }
        
        for job_type, keywords in job_keywords.items():
            for keyword in keywords:
                if keyword.lower() in question.lower():
                    job_types.append(job_type)
                    break
        
        return list(set(job_types))
    
    def _extract_amounts(self, question: str) -> List[str]:
        """Extract amounts/numbers"""
        amounts = []
        
        # Number patterns
        amount_patterns = [
            r'\d{1,3}(?:,\d{3})*\s*‡∏ö‡∏≤‡∏ó',
            r'\$\d{1,3}(?:,\d{3})*',
            r'\d+\s*‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà'
        ]
        
        for pattern in amount_patterns:
            matches = re.findall(pattern, question)
            amounts.extend(matches)
        
        return amounts
    
    def _clean_and_validate_entities(self, entities: Dict[str, Any]) -> Dict[str, Any]:
        """Clean and validate extracted entities"""
        cleaned = {}
        
        for key, value in entities.items():
            if isinstance(value, list):
                # Remove duplicates and empty values
                cleaned_list = list(set([v for v in value if v and str(v).strip()]))
                
                # Sort for consistency
                if key in ['years', 'months']:
                    cleaned_list.sort()
                
                cleaned[key] = cleaned_list
            else:
                cleaned[key] = value
        
        return cleaned

    # =================================================================
    # UTILITY METHODS
    # =================================================================
    
    def get_intent_confidence_report(self, question: str) -> Dict[str, Any]:
        """Get detailed confidence report for debugging"""
        result = self.detect_intent_and_entities(question)
        
        # Sort scores by value
        sorted_scores = sorted(result['scores'].items(), key=lambda x: x[1], reverse=True)
        
        return {
            'question': question,
            'detected_intent': result['intent'],
            'confidence': result['confidence'],
            'entities': result['entities'],
            'all_scores': sorted_scores,
            'top_3_intents': sorted_scores[:3]
        }
    
    def test_intent_accuracy(self, test_cases: List[Tuple[str, str]]) -> Dict[str, Any]:
        """Test intent detection accuracy with known cases"""
        correct = 0
        total = len(test_cases)
        results = []
        
        for question, expected_intent in test_cases:
            result = self.detect_intent_and_entities(question)
            detected = result['intent']
            confidence = result['confidence']
            
            is_correct = detected == expected_intent
            if is_correct:
                correct += 1
            
            results.append({
                'question': question,
                'expected': expected_intent,
                'detected': detected,
                'confidence': confidence,
                'correct': is_correct
            })
        
        return {
            'accuracy': correct / total if total > 0 else 0,
            'correct_count': correct,
            'total_count': total,
            'results': results
        }
  