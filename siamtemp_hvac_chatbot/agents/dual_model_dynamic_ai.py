import os
import re
import json
import asyncio
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, date
from decimal import Decimal

logger = logging.getLogger(__name__)

class SimplifiedDatabaseHandler:
    """Simplified database handler for HVAC data"""
    
    def __init__(self):
        self.connection_cache = {}
    
    def get_database_connection(self, tenant_id: str):
        """Get database connection for tenant"""
        try:
            import psycopg2
            
            # Database configuration mapping
            db_configs = {
                'company-a': {
                    'host': os.getenv('POSTGRES_HOST_COMPANY_A', 'postgres-company-a'),
                    'port': int(os.getenv('POSTGRES_PORT_COMPANY_A', '5432')),
                    'database': os.getenv('POSTGRES_DB_COMPANY_A', 'siamtemp_company_a'),
                    'user': os.getenv('POSTGRES_USER_COMPANY_A', 'postgres'),
                    'password': os.getenv('POSTGRES_PASSWORD_COMPANY_A', 'password123')
                },
                'company-b': {
                    'host': os.getenv('POSTGRES_HOST_COMPANY_B', 'postgres-company-b'),
                    'port': int(os.getenv('POSTGRES_PORT_COMPANY_B', '5432')),
                    'database': os.getenv('POSTGRES_DB_COMPANY_B', 'siamtemp_company_b'),
                    'user': os.getenv('POSTGRES_USER_COMPANY_B', 'postgres'),
                    'password': os.getenv('POSTGRES_PASSWORD_COMPANY_B', 'password123')
                },
                'company-c': {
                    'host': os.getenv('POSTGRES_HOST_COMPANY_C', 'postgres-company-c'),
                    'port': int(os.getenv('POSTGRES_PORT_COMPANY_C', '5432')),
                    'database': os.getenv('POSTGRES_DB_COMPANY_C', 'siamtemp_company_c'),
                    'user': os.getenv('POSTGRES_USER_COMPANY_C', 'postgres'),
                    'password': os.getenv('POSTGRES_PASSWORD_COMPANY_C', 'password123')
                }
            }
            
            config = db_configs.get(tenant_id, db_configs['company-a'])
            
            connection = psycopg2.connect(**config)
            return connection
            
        except Exception as e:
            logger.error(f"Database connection failed for {tenant_id}: {e}")
            return None
    
    async def execute_query(self, sql: str, tenant_id: str) -> List[Dict[str, Any]]:
        """Execute SQL query and return results"""
        try:
            conn = self.get_database_connection(tenant_id)
            if not conn:
                return []
            
            with conn.cursor() as cursor:
                cursor.execute(sql)
                columns = [desc[0] for desc in cursor.description]
                rows = cursor.fetchall()
                
                results = []
                for row in rows:
                    result = {}
                    for i, value in enumerate(row):
                        # Handle different data types
                        if isinstance(value, Decimal):
                            result[columns[i]] = float(value)
                        elif isinstance(value, (date, datetime)):
                            result[columns[i]] = str(value)
                        else:
                            result[columns[i]] = value
                    results.append(result)
                
                conn.close()
                return results
                
        except Exception as e:
            logger.error(f"Query execution failed: {e}")
            return []

class SimplifiedOllamaClient:
    """Simplified Ollama client for AI requests"""
    
    def __init__(self):
        self.base_url = os.getenv('OLLAMA_BASE_URL', 'http://52.74.36.160:12434')
        self.timeout = int(os.getenv('OLLAMA_TIMEOUT', '60'))
    
    async def generate_response(self, model: str, prompt: str, temperature: float = 0.7) -> str:
        """Generate response from Ollama model"""
        try:
            import aiohttp
            
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": 2000
                }
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout)) as session:
                async with session.post(f"{self.base_url}/api/generate", json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result.get('response', '')
                    else:
                        logger.error(f"Ollama API error: {response.status}")
                        return ""
                        
        except Exception as e:
            logger.error(f"Ollama request failed: {e}")
            return ""

class DualModelDynamicAISystem:
    """Core Dual-Model AI System for HVAC chatbot with Performance Optimization"""
    
    def __init__(self):
        # Initialize components
        self.db_handler = SimplifiedDatabaseHandler()
        self.ollama_client = SimplifiedOllamaClient()
        
        # Model configuration
        self.SQL_MODEL = "mannix/defog-llama3-sqlcoder-8b:latest"
        self.NL_MODEL = "llama3.1:8b"
        
        # Performance optimization settings
        self.sql_cache = {}  # Cache SQL patterns à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸šà¹ˆà¸­à¸¢
        self.schema_cache = {}  # Cache database schema
        self.enable_parallel = True
        self.timeout_sql = 20  # à¹€à¸žà¸´à¹ˆà¸¡ timeout SQL generation à¹€à¸›à¹‡à¸™ 20 à¸§à¸´à¸™à¸²à¸—à¸µ
        self.timeout_nl = 10   # à¹€à¸žà¸´à¹ˆà¸¡ timeout NL generation à¹€à¸›à¹‡à¸™ 10 à¸§à¸´à¸™à¸²à¸—à¸µ
        
        # HVAC Business Knowledge
        self.hvac_context = {
            'tables': {
                'sales2024': 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‡à¸²à¸™à¸šà¸£à¸´à¸à¸²à¸£à¹à¸¥à¸°à¸à¸²à¸£à¸‚à¸²à¸¢à¸›à¸µ 2024',
                'sales2023': 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‡à¸²à¸™à¸šà¸£à¸´à¸à¸²à¸£à¹à¸¥à¸°à¸à¸²à¸£à¸‚à¸²à¸¢à¸›à¸µ 2023',
                'sales2022': 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‡à¸²à¸™à¸šà¸£à¸´à¸à¸²à¸£à¹à¸¥à¸°à¸à¸²à¸£à¸‚à¸²à¸¢à¸›à¸µ 2022',
                'sales2025': 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‡à¸²à¸™à¸šà¸£à¸´à¸à¸²à¸£à¹à¸¥à¸°à¸à¸²à¸£à¸‚à¸²à¸¢à¸›à¸µ 2025',
                'spare_part': 'à¸„à¸¥à¸±à¸‡à¸­à¸°à¹„à¸«à¸¥à¹ˆà¸«à¸¥à¸±à¸',
                'spare_part2': 'à¸„à¸¥à¸±à¸‡à¸­à¸°à¹„à¸«à¸¥à¹ˆà¸ªà¸³à¸£à¸­à¸‡',
                'work_force': 'à¸à¸²à¸£à¸ˆà¸±à¸”à¸—à¸µà¸¡à¸‡à¸²à¸™à¹à¸¥à¸°à¹à¸œà¸™à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™'
            },
            'keywords': {
                'job_quotation': ['à¹€à¸ªà¸™à¸­à¸£à¸²à¸„à¸²', 'à¸‡à¸²à¸™', 'standard', 'à¸¡à¸²à¸•à¸£à¸à¸²à¸™', 'quotation', 'job_no'],
                'spare_parts': ['à¸­à¸°à¹„à¸«à¸¥à¹ˆ', 'spare', 'parts', 'motor', 'chiller', 'compressor', 'product_name'],
                'sales_analysis': ['à¸à¸²à¸£à¸‚à¸²à¸¢', 'à¸¢à¸­à¸”à¸‚à¸²à¸¢', 'à¸£à¸²à¸¢à¹„à¸”à¹‰', 'à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ', 'service_contact_'],
                'work_summary': ['à¸‡à¸²à¸™', 'work', 'à¸ªà¸£à¸¸à¸›', 'summary', 'à¸—à¸µà¸¡', 'team'],
                'brands': ['hitachi', 'daikin', 'mitsubishi', 'carrier', 'trane', 'york', 'lg']
            },
            'year_mapping': {
                '2565': '2022', '2566': '2023', '2567': '2024', '2568': '2025',
                'à¸›à¸µ2565': '2022', 'à¸›à¸µ2566': '2023', 'à¸›à¸µ2567': '2024', 'à¸›à¸µ2568': '2025'
            }
        }
        
        # Performance tracking
        self.stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'sql_generated': 0,
            'nl_responses': 0,
            'cache_hits': 0,
            'avg_response_time': 0.0
        }
        
        logger.info("Dual-Model Dynamic AI System with Performance Optimization initialized successfully")
    
    def _classify_question_intent(self, question: str) -> str:
        """Enhanced question intent classification with priority-based matching"""
        question_lower = question.lower()
        
        # HIGH PRIORITY: Overhaul analysis (must be checked first before spare_parts)
        overhaul_keywords = ['overhaul', 'à¸¢à¸à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡', 'à¸‹à¹ˆà¸­à¸¡à¹ƒà¸«à¸à¹ˆ', 'à¸£à¸²à¸¢à¸‡à¸²à¸™à¸¢à¸­à¸”à¸‚à¸²à¸¢ overhaul', 
                            'à¸¢à¸­à¸”à¸‚à¸²à¸¢ overhaul', 'à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ overhaul']
        if any(keyword in question_lower for keyword in overhaul_keywords):
            logger.info(f"ðŸ”§ Classified as OVERHAUL_ANALYSIS: found '{[k for k in overhaul_keywords if k in question_lower]}'")
            return 'overhaul_analysis'
        
        # HIGH PRIORITY: Sales analysis with year patterns
        sales_keywords = ['à¸£à¸²à¸¢à¸‡à¸²à¸™à¸¢à¸­à¸”à¸‚à¸²à¸¢', 'à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸à¸²à¸£à¸‚à¸²à¸¢', 'à¸¢à¸­à¸”à¸‚à¸²à¸¢', 'à¸£à¸²à¸¢à¸‡à¸²à¸™', 'analysis', 
                         'à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸¢à¸­à¸”à¸‚à¸²à¸¢', 'à¸ªà¸£à¸¸à¸›à¸¢à¸­à¸”à¸‚à¸²à¸¢']
        if any(keyword in question_lower for keyword in sales_keywords):
            logger.info(f"ðŸ“Š Classified as SALES_ANALYSIS: found '{[k for k in sales_keywords if k in question_lower]}'")
            return 'sales_analysis'
        
        # Customer-specific queries
        customer_keywords = ['à¸šà¸£à¸´à¸©à¸±à¸—', 'à¸¥à¸¹à¸à¸„à¹‰à¸²', 'customer', 'à¸ˆà¸³à¸™à¸§à¸™à¸¥à¸¹à¸à¸„à¹‰à¸²', 'à¸›à¸£à¸°à¸§à¸±à¸•à¸´à¸¥à¸¹à¸à¸„à¹‰à¸²', 'à¸‹à¸·à¹‰à¸­à¸‚à¸²à¸¢']
        if any(keyword in question_lower for keyword in customer_keywords):
            return 'customer_analysis'
        
        # Work schedule and planning queries  
        schedule_keywords = ['à¹à¸œà¸™à¸‡à¸²à¸™', 'à¸§à¸²à¸‡à¹à¸œà¸™', 'schedule', 'à¸§à¸±à¸™à¸—à¸µà¹ˆ', 'à¹€à¸”à¸·à¸­à¸™', 'work_force', 
                            'à¸¡à¸à¸£à¸²à¸„à¸¡', 'à¸à¸¸à¸¡à¸ à¸²à¸žà¸±à¸™à¸˜à¹Œ', 'à¸¡à¸µà¸™à¸²à¸„à¸¡', 'à¹€à¸¡à¸©à¸²à¸¢à¸™', 'à¸žà¸¤à¸©à¸ à¸²à¸„à¸¡', 'à¸¡à¸´à¸–à¸¸à¸™à¸²à¸¢à¸™',
                            'à¸à¸£à¸à¸Žà¸²à¸„à¸¡', 'à¸ªà¸´à¸‡à¸«à¸²à¸„à¸¡', 'à¸à¸±à¸™à¸¢à¸²à¸¢à¸™', 'à¸•à¸¸à¸¥à¸²à¸„à¸¡', 'à¸žà¸¤à¸¨à¸ˆà¸´à¸à¸²à¸¢à¸™', 'à¸˜à¸±à¸™à¸§à¸²à¸„à¸¡']
        if any(keyword in question_lower for keyword in schedule_keywords):
            return 'work_schedule'
            
        # Specific model/part number queries (exclude compressor from here)
        model_keywords = ['model', 'rcug', 'ekac', 'ek258', 'à¸£à¸²à¸„à¸²à¸­à¸°à¹„à¸«à¸¥à¹ˆ', 'air cooled', 'water cooled']
        if any(keyword in question_lower for keyword in model_keywords):
            return 'specific_parts'
        
        # Standard job quotations  
        job_keywords = ['à¹€à¸ªà¸™à¸­à¸£à¸²à¸„à¸²', 'à¸‡à¸²à¸™', 'quotation', 'standard', 'à¸ªà¸£à¸¸à¸›à¸‡à¸²à¸™']
        if any(keyword in question_lower for keyword in job_keywords):
            return 'job_quotation'
        
        # LOWER PRIORITY: Spare parts detection (moved to end to avoid conflicts)
        spare_parts_keywords = ['à¸­à¸°à¹„à¸«à¸¥à¹ˆ', 'spare', 'parts', 'motor', 'chiller', 
                               'hitachi', 'daikin', 'mitsubishi', 'carrier', 'trane', 'york', 
                               'fan', 'circuit', 'board', 'transformer', 'valve']
        if any(keyword in question_lower for keyword in spare_parts_keywords):
            # Additional check to avoid overhaul conflicts
            if 'overhaul' not in question_lower and 'à¸¢à¸­à¸”à¸‚à¸²à¸¢' not in question_lower:
                return 'spare_parts'
            
        return 'general'
    
    def _convert_thai_years_and_months(self, question: str) -> Tuple[str, Dict[str, Any]]:
        """Convert Thai years to international years and extract month info"""
        converted = question
        month_info = {}
        
        # Year mappings
        year_mappings = {
            '2565': '2022', '2566': '2023', '2567': '2024', '2568': '2025',
            'à¸›à¸µ2565': '2022', 'à¸›à¸µ2566': '2023', 'à¸›à¸µ2567': '2024', 'à¸›à¸µ2568': '2025',
            'à¸›à¸µ 2565': '2022', 'à¸›à¸µ 2566': '2023', 'à¸›à¸µ 2567': '2024', 'à¸›à¸µ 2568': '2025'
        }
        
        # Month mappings for job_no pattern matching
        month_mappings = {
            'à¸¡à¸à¸£à¸²à¸„à¸¡': '01', 'à¸à¸¸à¸¡à¸ à¸²à¸žà¸±à¸™à¸˜à¹Œ': '02', 'à¸¡à¸µà¸™à¸²à¸„à¸¡': '03',
            'à¹€à¸¡à¸©à¸²à¸¢à¸™': '04', 'à¸žà¸¤à¸©à¸ à¸²à¸„à¸¡': '05', 'à¸¡à¸´à¸–à¸¸à¸™à¸²à¸¢à¸™': '06',
            'à¸à¸£à¸à¸Žà¸²à¸„à¸¡': '07', 'à¸ªà¸´à¸‡à¸«à¸²à¸„à¸¡': '08', 'à¸à¸±à¸™à¸¢à¸²à¸¢à¸™': '09', 
            'à¸•à¸¸à¸¥à¸²à¸„à¸¡': '10', 'à¸žà¸¤à¸¨à¸ˆà¸´à¸à¸²à¸¢à¸™': '11', 'à¸˜à¸±à¸™à¸§à¸²à¸„à¸¡': '12'
        }
        
        # Apply year conversions
        for thai_year, intl_year in year_mappings.items():
            if thai_year in question:
                converted = converted.replace(thai_year, intl_year)
                # Extract the last 2 digits for job_no pattern
                month_info['year_short'] = intl_year[-2:]  # '2022' -> '22'
                month_info['year_full'] = intl_year
        
        # Extract month information
        for thai_month, month_code in month_mappings.items():
            if thai_month in question:
                month_info['month'] = month_code
                month_info['month_thai'] = thai_month
        
        # Log conversion if any changes were made
        if converted != question or month_info:
            logger.info(f"Thai conversion: '{question}' â†’ '{converted}', month_info: {month_info}")
        
        return converted, month_info
    
    async def _generate_sql_query(self, question: str, intent: str, tenant_id: str) -> str:
        """Generate SQL query using specialized SQL model with enhanced Thai support"""
        try:
            self.stats['sql_generated'] += 1
            
            # Check cache first
            cache_key = f"{intent}_{hash(question.lower().strip())}"
            if cache_key in self.sql_cache:
                logger.info("Using cached SQL pattern")
                self.stats['cache_hits'] += 1
                return self.sql_cache[cache_key]
            
            # Convert Thai years and extract month info
            converted_question, month_info = self._convert_thai_years_and_months(question)
            
            # Build enhanced context for job_no pattern matching
            job_pattern_help = ""
            if month_info:
                if 'year_short' in month_info and 'month' in month_info:
                    job_pattern_help = f"\nSPECIAL: For {month_info.get('month_thai', 'unknown month')} {month_info.get('year_full', 'unknown year')}, search job_no pattern like '%{month_info['year_short']}-{month_info['month']}-%'"
            
            # Enhanced prompt with comprehensive HVAC business examples
            sql_prompt = f"""Generate PostgreSQL query for HVAC business data analysis.

DATABASE SCHEMA:
- sales2022,2023,2024,2025: id, job_no, customer_name, description, overhaul_, replacement, service_contact_
- spare_part, spare_part2: id, wh, product_code, product_name, unit, balance, unit_price, description
- work_force: id, date, customer, project, detail, service_group

IMPORTANT DATA TYPES: 
- overhaul_, replacement, service_contact_, unit_price are TEXT fields - ALWAYS use CAST(field AS NUMERIC)
- For revenue analysis, use COALESCE to handle NULL values

JOB_NO PATTERN: JAE[year]-[month]-[sequence]-[type]
BUSINESS CONTEXT: HVAC service company with overhaul, maintenance, and spare parts sales{job_pattern_help}

BUSINESS QUERY PATTERNS:

1. OVERHAUL REVENUE ANALYSIS (Most Important):
"à¸£à¸²à¸¢à¸‡à¸²à¸™à¸¢à¸­à¸”à¸‚à¸²à¸¢ overhaul compressor à¸›à¸µ 2567-2568"
```sql
SELECT year, jobs, overhaul_revenue, service_revenue, total_revenue FROM (
    SELECT '2024' as year, COUNT(*) as jobs, 
           SUM(CAST(overhaul_ AS NUMERIC)) as overhaul_revenue,
           SUM(CAST(service_contact_ AS NUMERIC)) as service_revenue,
           SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0)) as total_revenue
    FROM sales2024 WHERE LOWER(description) LIKE '%overhaul%' AND LOWER(description) LIKE '%compressor%'
    UNION ALL
    SELECT '2025' as year, COUNT(*) as jobs,
           SUM(CAST(overhaul_ AS NUMERIC)) as overhaul_revenue,
           SUM(CAST(service_contact_ AS NUMERIC)) as service_revenue, 
           SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0)) as total_revenue
    FROM sales2025 WHERE LOWER(description) LIKE '%overhaul%' AND LOWER(description) LIKE '%compressor%'
) sub ORDER BY year;
```

2. YEARLY REVENUE COMPARISON:
"à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸¢à¸­à¸”à¸‚à¸²à¸¢à¸›à¸µ 2567 à¸à¸±à¸š 2568"
```sql
SELECT year, total_jobs, total_revenue FROM (
    SELECT '2024' as year, COUNT(*) as total_jobs, 
           SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0) + COALESCE(CAST(replacement AS NUMERIC), 0)) as total_revenue
    FROM sales2024 WHERE (overhaul_ IS NOT NULL OR service_contact_ IS NOT NULL OR replacement IS NOT NULL)
    UNION ALL
    SELECT '2025' as year, COUNT(*) as total_jobs,
           SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0) + COALESCE(CAST(replacement AS NUMERIC), 0)) as total_revenue
    FROM sales2025 WHERE (overhaul_ IS NOT NULL OR service_contact_ IS NOT NULL OR replacement IS NOT NULL)
) sub ORDER BY year;
```

3. CUSTOMER ANALYSIS:
"à¸šà¸£à¸´à¸©à¸±à¸— ABC à¸¡à¸µà¸à¸²à¸£à¸‹à¸·à¹‰à¸­à¸‚à¸²à¸¢à¸¢à¹‰à¸­à¸™à¸«à¸¥à¸±à¸‡ à¸¡à¸¹à¸¥à¸„à¹ˆà¸²à¹€à¸—à¹ˆà¸²à¹„à¸«à¸£à¹ˆ"
```sql
SELECT job_no, description, CAST(service_contact_ AS NUMERIC) as value, CAST(overhaul_ AS NUMERIC) as overhaul FROM sales2024 WHERE customer_name ILIKE '%ABC%'
UNION ALL
SELECT job_no, description, CAST(service_contact_ AS NUMERIC) as value, CAST(overhaul_ AS NUMERIC) as overhaul FROM sales2025 WHERE customer_name ILIKE '%ABC%'
ORDER BY job_no;
```

4. SPARE PARTS SEARCH:
"à¸£à¸²à¸„à¸²à¸­à¸°à¹„à¸«à¸¥à¹ˆ Hitachi motor"
```sql
SELECT product_code, product_name, CAST(unit_price AS NUMERIC) as price, balance, description FROM spare_part
WHERE LOWER(product_name) LIKE '%hitachi%' AND LOWER(product_name) LIKE '%motor%'
UNION ALL
SELECT product_code, product_name, CAST(unit_price AS NUMERIC) as price, balance, description FROM spare_part2
WHERE LOWER(product_name) LIKE '%hitachi%' AND LOWER(product_name) LIKE '%motor%'
ORDER BY price DESC NULLS LAST;
```

Question: {converted_question}
Original Question: {question}  
Intent Category: {intent}

CRITICAL RULES:
- For overhaul/revenue analysis: ALWAYS use UNION ALL to combine multiple years
- ALWAYS use CAST(field AS NUMERIC) for price calculations
- Use COALESCE for NULL handling in revenue calculations
- Use ILIKE for case-insensitive text search
- Include ORDER BY for consistent results

Generate the most appropriate SQL query:"""
            
            # Use timeout for faster response
            sql_response = await asyncio.wait_for(
                self.ollama_client.generate_response(
                    model=self.SQL_MODEL,
                    prompt=sql_prompt,
                    temperature=0.1  # Low temperature for precise SQL
                ),
                timeout=self.timeout_sql
            )
            
            # Clean up the SQL response
            sql_query = self._clean_sql_response(sql_response)
            
            # Basic SQL validation
            if not sql_query.upper().startswith(('SELECT', 'WITH')):
                logger.warning(f"Generated SQL doesn't start with SELECT: {sql_query}")
                return self._get_fallback_sql_with_context(intent, month_info)
            
            # Cache for future use
            self.sql_cache[cache_key] = sql_query
            
            return sql_query
            
        except asyncio.TimeoutError:
            logger.warning("SQL generation timeout, using contextual fallback")
            return self._get_fallback_sql_with_context(intent, {})
        except Exception as e:
            logger.error(f"SQL generation failed: {e}")
            return self._get_fallback_sql(intent)
    
    def _clean_sql_response(self, sql_response: str) -> str:
        """Clean up SQL response from AI model"""
        sql_query = sql_response.strip()
        
        # Remove markdown code blocks
        if sql_query.startswith('```sql'):
            sql_query = sql_query[6:]
        if sql_query.startswith('```'):
            sql_query = sql_query[3:]
        if sql_query.endswith('```'):
            sql_query = sql_query[:-3]
        
        return sql_query.strip()
    
    def _get_fallback_sql(self, intent: str) -> str:
        """Get fallback SQL queries based on real schema structure"""
        fallback_queries = {
            'job_quotation': """SELECT job_no, customer_name, description, 
                               CAST(service_contact_ AS NUMERIC) as service_price,
                               CAST(overhaul_ AS NUMERIC) as overhaul,
                               CAST(replacement AS NUMERIC) as replacement
                               FROM sales2024 
                               WHERE description IS NOT NULL 
                               ORDER BY CAST(service_contact_ AS NUMERIC) DESC NULLS LAST 
                               LIMIT 10""",
            
            'spare_parts': """SELECT product_code, product_name, wh,
                             CAST(unit_price AS NUMERIC) as price, balance, description
                             FROM spare_part 
                             WHERE product_name IS NOT NULL 
                             UNION ALL
                             SELECT product_code, product_name, wh,
                             CAST(unit_price AS NUMERIC) as price, balance, description  
                             FROM spare_part2
                             WHERE product_name IS NOT NULL
                             ORDER BY price DESC NULLS LAST
                             LIMIT 15""",
            
            'sales_analysis': """SELECT '2024' as year, COUNT(*) as jobs, 
                                SUM(CAST(overhaul_ AS NUMERIC)) as overhaul_revenue,
                                SUM(CAST(service_contact_ AS NUMERIC)) as service_revenue,
                                SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0)) as total_revenue
                                FROM sales2024 WHERE service_contact_ IS NOT NULL OR overhaul_ IS NOT NULL
                                UNION ALL
                                SELECT '2025' as year, COUNT(*) as jobs,
                                SUM(CAST(overhaul_ AS NUMERIC)) as overhaul_revenue,
                                SUM(CAST(service_contact_ AS NUMERIC)) as service_revenue,
                                SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0)) as total_revenue  
                                FROM sales2025 WHERE service_contact_ IS NOT NULL OR overhaul_ IS NOT NULL
                                ORDER BY year""",
            
            'overhaul_analysis': """SELECT '2024' as year, COUNT(*) as jobs, 
                                   SUM(CAST(overhaul_ AS NUMERIC)) as overhaul_revenue,
                                   SUM(CAST(service_contact_ AS NUMERIC)) as service_revenue,
                                   SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0)) as total_revenue
                                   FROM sales2024 WHERE LOWER(description) LIKE '%overhaul%' AND LOWER(description) LIKE '%compressor%'
                                   UNION ALL
                                   SELECT '2025' as year, COUNT(*) as jobs,
                                   SUM(CAST(overhaul_ AS NUMERIC)) as overhaul_revenue,
                                   SUM(CAST(service_contact_ AS NUMERIC)) as service_revenue,
                                   SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0)) as total_revenue
                                   FROM sales2025 WHERE LOWER(description) LIKE '%overhaul%' AND LOWER(description) LIKE '%compressor%'
                                   ORDER BY year""",
            
            'work_summary': """SELECT date, customer, detail, service_group
                              FROM work_force 
                              WHERE detail IS NOT NULL 
                              ORDER BY date DESC NULLS LAST 
                              LIMIT 10""",
            
            'general': "SELECT COUNT(*) as total_records FROM sales2024"
        }
        
        return fallback_queries.get(intent, fallback_queries['general'])
    
    def _get_fallback_sql_with_context(self, intent: str, month_info: Dict[str, Any]) -> str:
        """Get contextual fallback SQL queries with improved overhaul detection"""
        
        # Special handling for overhaul analysis
        if intent == 'overhaul_analysis' or intent == 'sales_analysis':
            return """SELECT '2024' as year, COUNT(*) as jobs, 
                     SUM(CAST(overhaul_ AS NUMERIC)) as overhaul_revenue,
                     SUM(CAST(service_contact_ AS NUMERIC)) as service_revenue,
                     SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0)) as total_revenue
                     FROM sales2024 WHERE LOWER(description) LIKE '%overhaul%' AND LOWER(description) LIKE '%compressor%'
                     UNION ALL
                     SELECT '2025' as year, COUNT(*) as jobs,
                     SUM(CAST(overhaul_ AS NUMERIC)) as overhaul_revenue,
                     SUM(CAST(service_contact_ AS NUMERIC)) as service_revenue, 
                     SUM(COALESCE(CAST(overhaul_ AS NUMERIC), 0) + COALESCE(CAST(service_contact_ AS NUMERIC), 0)) as total_revenue
                     FROM sales2025 WHERE LOWER(description) LIKE '%overhaul%' AND LOWER(description) LIKE '%compressor%'
                     ORDER BY year"""
        
        # If we have month and year info, use job_no pattern matching
        if month_info.get('year_short') and month_info.get('month'):
            year_short = month_info['year_short']
            month = month_info['month']
            table = f"sales20{year_short}"
            
            return f"""SELECT job_no, customer_name, description, 
                      CAST(service_contact_ AS NUMERIC) as revenue,
                      CAST(overhaul_ AS NUMERIC) as overhaul,
                      CAST(replacement AS NUMERIC) as replacement
                      FROM {table}
                      WHERE job_no LIKE '%{year_short}-{month}-%'
                      AND (service_contact_ IS NOT NULL OR overhaul_ IS NOT NULL OR replacement IS NOT NULL)
                      ORDER BY job_no"""
        
        # Otherwise use standard fallback
        return self._get_fallback_sql(intent)
    
    async def _generate_natural_response(self, question: str, sql_results: List[Dict], sql_query: str, tenant_id: str) -> str:
        """Generate natural language response using NL model with smart optimization"""
        try:
            self.stats['nl_responses'] += 1
            
            if not sql_results:
                return f"à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸«à¸£à¸±à¸šà¸„à¸³à¸–à¸²à¸¡: {question}"
            
            # Smart response strategy based on data size
            if len(sql_results) > 20:
                # For large datasets, use structured response immediately to save time
                logger.info("Large dataset detected, using structured response for speed")
                return self._get_structured_response(question, sql_results)
            
            # Create concise data summary for faster processing
            data_summary = self._create_smart_summary(sql_results, question)
            
            # Use ultra-short prompt for speed
            nl_prompt = f"""à¸„à¸³à¸–à¸²à¸¡: {question}
à¸‚à¹‰à¸­à¸¡à¸¹à¸¥: {data_summary}

à¸•à¸­à¸šà¸ªà¸±à¹‰à¸™à¹† à¹ƒà¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢:"""
            
            # Use shorter timeout for faster response
            nl_response = await asyncio.wait_for(
                self.ollama_client.generate_response(
                    model=self.NL_MODEL,
                    prompt=nl_prompt,
                    temperature=0.5  # Lower temperature for faster, more focused response
                ),
                timeout=6  # Very short timeout
            )
            
            return nl_response.strip() if nl_response else self._get_universal_structured_response(question, sql_results)
            
        except asyncio.TimeoutError:
            logger.info("NL generation timeout (6s), using universal structured response")
            return self._get_universal_structured_response(question, sql_results)
        except Exception as e:
            logger.error(f"NL response generation failed: {e}")
            return self._get_universal_structured_response(question, sql_results)
    
    def _create_smart_summary(self, results: List[Dict], question: str) -> str:
        """Create smart data summary based on question type"""
        if not results:
            return "à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥"
        
        # For sales analysis - focus on numbers
        if any(word in question.lower() for word in ['à¸¢à¸­à¸”à¸‚à¸²à¸¢', 'à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š', 'à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ']):
            if len(results) <= 5:
                return f"Sales data: {results}"
            else:
                return f"Sales summary: {len(results)} records, sample: {results[:2]}"
        
        # For spare parts - focus on product info  
        elif any(word in question.lower() for word in ['à¸­à¸°à¹„à¸«à¸¥à¹ˆ', 'spare', 'parts']):
            if len(results) <= 10:
                return f"Parts found: {results[:5]}"
            else:
                return f"Found {len(results)} parts, top items: {results[:3]}"
        
        # Generic summary
        else:
            return f"{len(results)} records: {results[:2] if len(results) > 2 else results}"
    
    def _get_optimized_structured_response(self, question: str, results: List[Dict]) -> str:
        """Enhanced structured response with complete business data"""
        if not results:
            return f"à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸«à¸£à¸±à¸šà¸„à¸³à¸–à¸²à¸¡: {question}"
        
        # Overhaul analysis response
        if any(word in question.lower() for word in ['overhaul', 'à¸¢à¸à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡', 'compressor']):
            response = f"à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸‡à¸²à¸™ Overhaul: {question}\n\nà¸žà¸š {len(results)} à¸£à¸²à¸¢à¸à¸²à¸£\n\n"
            
            total_value = 0
            for i, result in enumerate(results, 1):
                job_no = result.get('job_no', 'N/A')
                customer = result.get('customer_name', 'N/A')
                description = result.get('description', 'N/A')
                
                # Try different price fields
                price = (result.get('total_overhaul') or 
                        result.get('overhaul_cost') or 
                        result.get('overhaul_') or 
                        result.get('service_contact_') or 0)
                
                if price:
                    total_value += float(price)
                    response += f"{i}. {job_no}\n   à¸¥à¸¹à¸à¸„à¹‰à¸²: {customer}\n   à¸‡à¸²à¸™: {description}\n   à¸¡à¸¹à¸¥à¸„à¹ˆà¸²: {price:,.0f} à¸šà¸²à¸—\n\n"
                else:
                    response += f"{i}. {job_no}\n   à¸¥à¸¹à¸à¸„à¹‰à¸²: {customer}\n   à¸‡à¸²à¸™: {description}\n   à¸¡à¸¹à¸¥à¸„à¹ˆà¸²: à¹„à¸¡à¹ˆà¸£à¸°à¸šà¸¸\n\n"
            
            if total_value > 0:
                response += f"à¸£à¸§à¸¡à¸¡à¸¹à¸¥à¸„à¹ˆà¸²à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {total_value:,.0f} à¸šà¸²à¸—"
            else:
                response += "à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸: à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¡à¸¹à¸¥à¸„à¹ˆà¸²à¸‡à¸²à¸™à¹ƒà¸™à¸£à¸°à¸šà¸š à¸­à¸²à¸ˆà¸•à¹‰à¸­à¸‡à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡"
            
            return response
        
        # Sales analysis response
        elif any(word in question.lower() for word in ['à¸¢à¸­à¸”à¸‚à¸²à¸¢', 'à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š', 'à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ']):
            response = f"à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸¢à¸­à¸”à¸‚à¸²à¸¢: {question}\n\n"
            
            total_jobs = 0
            total_revenue = 0
            
            for result in results:
                year = result.get('year', 'N/A')
                jobs = result.get('jobs', 0) 
                revenue = result.get('revenue') or result.get('total_overhaul') or 0
                avg_per_job = revenue / jobs if jobs > 0 else 0
                
                total_jobs += jobs
                total_revenue += revenue
                
                if revenue > 0:
                    response += f"à¸›à¸µ {year}: {jobs:,} à¸‡à¸²à¸™, à¸£à¸²à¸¢à¹„à¸”à¹‰ {revenue:,.0f} à¸šà¸²à¸— (à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ {avg_per_job:,.0f} à¸šà¸²à¸—/à¸‡à¸²à¸™)\n"
                else:
                    response += f"à¸›à¸µ {year}: {jobs:,} à¸‡à¸²à¸™, à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸²à¸¢à¹„à¸”à¹‰\n"
            
            if total_revenue > 0:
                response += f"\nà¸ªà¸£à¸¸à¸›à¸£à¸§à¸¡: {total_jobs:,} à¸‡à¸²à¸™, à¸£à¸²à¸¢à¹„à¸”à¹‰à¸£à¸§à¸¡ {total_revenue:,.0f} à¸šà¸²à¸—"
            else:
                response += f"\nà¸ªà¸£à¸¸à¸›à¸£à¸§à¸¡: {total_jobs:,} à¸‡à¸²à¸™, à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸²à¸¢à¹„à¸”à¹‰à¹„à¸¡à¹ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ"
            
            return response
        
        # Spare parts response  
        elif any(word in question.lower() for word in ['à¸­à¸°à¹„à¸«à¸¥à¹ˆ', 'spare', 'parts']):
            response = f"à¸œà¸¥à¸à¸²à¸£à¸„à¹‰à¸™à¸«à¸²à¸­à¸°à¹„à¸«à¸¥à¹ˆ: {question}\n\nà¸žà¸š {len(results)} à¸£à¸²à¸¢à¸à¸²à¸£\n\n"
            
            for i, result in enumerate(results[:5], 1):
                name = result.get('product_name', 'N/A')
                code = result.get('product_code', 'N/A')
                price = result.get('price') or result.get('unit_price', 0)
                balance = result.get('balance', 0)
                description = result.get('description', '')
                
                response += f"{i}. {name} ({code})\n"
                if price:
                    response += f"   à¸£à¸²à¸„à¸²: {price:,.0f} à¸šà¸²à¸—"
                if balance:
                    response += f" | à¸„à¸‡à¹€à¸«à¸¥à¸·à¸­: {balance} à¸Šà¸´à¹‰à¸™"
                if description:
                    response += f"\n   à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”: {description}"
                response += "\n\n"
            
            if len(results) > 5:
                response += f"... à¹à¸¥à¸°à¸­à¸µà¸ {len(results) - 5} à¸£à¸²à¸¢à¸à¸²à¸£"
            
            return response
        
        # Customer analysis response
        elif any(word in question.lower() for word in ['à¸¥à¸¹à¸à¸„à¹‰à¸²', 'customer', 'à¸šà¸£à¸´à¸©à¸±à¸—']):
            if len(results) == 1 and 'total_customers' in results[0]:
                # Customer count query
                count = results[0]['total_customers']
                return f"à¸ˆà¸³à¸™à¸§à¸™à¸¥à¸¹à¸à¸„à¹‰à¸²à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {count:,} à¸£à¸²à¸¢"
            else:
                # Customer details
                response = f"à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸¹à¸à¸„à¹‰à¸²: {question}\n\nà¸žà¸š {len(results)} à¸£à¸²à¸¢à¸à¸²à¸£\n\n"
                
                total_value = 0
                for i, result in enumerate(results[:10], 1):
                    job_no = result.get('job_no', 'N/A')
                    customer = result.get('customer_name', 'N/A')
                    description = result.get('description', 'N/A')
                    value = result.get('value') or result.get('service_contact_') or 0
                    
                    response += f"{i}. {job_no} | {customer}\n   à¸‡à¸²à¸™: {description}"
                    if value:
                        total_value += float(value)
                        response += f"\n   à¸¡à¸¹à¸¥à¸„à¹ˆà¸²: {value:,.0f} à¸šà¸²à¸—"
                    response += "\n\n"
                
                if total_value > 0:
                    response += f"à¸£à¸§à¸¡à¸¡à¸¹à¸¥à¸„à¹ˆà¸²à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {total_value:,.0f} à¸šà¸²à¸—"
                
                return response
        
        # Generic response
        else:
            response = f"à¸œà¸¥à¸à¸²à¸£à¸„à¹‰à¸™à¸«à¸²: {question}\n\nà¸žà¸š {len(results)} à¸£à¸²à¸¢à¸à¸²à¸£\n\n"
            for i, result in enumerate(results[:5], 1):
                response += f"{i}. "
                key_values = list(result.items())[:4]  # Show first 4 fields
                response += ", ".join(f"{k}: {v}" for k, v in key_values)
                response += "\n"
            
            if len(results) > 5:
                response += f"\n... à¹à¸¥à¸°à¸­à¸µà¸ {len(results) - 5} à¸£à¸²à¸¢à¸à¸²à¸£"
            
            return response
    
    def _get_structured_response(self, question: str, results: List[Dict]) -> str:
        """Generate structured response when NL model is not used"""
        return self._get_universal_structured_response(question, results)
    
    async def process_any_question(self, question: str, tenant_id: str) -> Dict[str, Any]:
        """Main entry point for processing questions with optimized dual-model approach"""
        start_time = time.time()
        self.stats['total_queries'] += 1
        
        try:
            logger.info(f"Processing question with Optimized Dual-Model AI: {question[:50]}...")
            
            # Step 1: Quick intent classification
            intent = self._classify_question_intent(question)
            logger.info(f"Question intent classified as: {intent}")
            
            # Step 2: Generate SQL query using specialized model (with caching)
            sql_query = await self._generate_sql_query(question, intent, tenant_id)
            logger.info(f"Generated SQL: {sql_query}")
            
            # Step 3: Execute SQL query
            sql_results = await self.db_handler.execute_query(sql_query, tenant_id)
            logger.info(f"SQL execution returned {len(sql_results)} results")
            
            # Step 4: Generate natural language response (optimized)
            if self.enable_parallel and len(sql_results) > 0:
                # Use optimized NL generation
                nl_response = await self._generate_natural_response(question, sql_results, sql_query, tenant_id)
            else:
                # Use universal structured response that adapts to data structure
                nl_response = self._get_universal_structured_response(question, sql_results)
            
            # Step 5: Prepare final response
            processing_time = time.time() - start_time
            success = len(sql_results) > 0
            
            if success:
                self.stats['successful_queries'] += 1
            
            # Update average response time
            total_time = self.stats['avg_response_time'] * (self.stats['total_queries'] - 1) + processing_time
            self.stats['avg_response_time'] = total_time / self.stats['total_queries']
            
            return {
                'answer': nl_response,
                'success': success,
                'sql_query': sql_query,
                'results_count': len(sql_results),
                'processing_time': processing_time,
                'ai_system_used': 'dual_model_optimized',
                'question_analysis': {
                    'intent': intent,
                    'tenant_id': tenant_id,
                    'optimization': 'enabled',
                    'cache_used': self.stats['cache_hits'] > 0,
                    'models_used': {
                        'sql_generation': self.SQL_MODEL,
                        'nl_generation': self.NL_MODEL
                    }
                },
                'models_used': {
                    'sql_model': self.SQL_MODEL,
                    'nl_model': self.NL_MODEL
                },
                'performance_stats': {
                    'avg_response_time': self.stats['avg_response_time'],
                    'cache_hit_rate': self.stats['cache_hits'] / max(self.stats['sql_generated'], 1) * 100,
                    'success_rate': self.stats['successful_queries'] / max(self.stats['total_queries'], 1) * 100
                }
            }
            
        except Exception as e:
            logger.error(f"Optimized dual-model processing failed: {e}")
            processing_time = time.time() - start_time
            
            return {
                'answer': f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥: {str(e)}\n\nà¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡à¸«à¸£à¸·à¸­à¸•à¸´à¸”à¸•à¹ˆà¸­à¸œà¸¹à¹‰à¸”à¸¹à¹à¸¥à¸£à¸°à¸šà¸š",
                'success': False,
                'sql_query': None,
                'results_count': 0,
                'processing_time': processing_time,
                'ai_system_used': 'dual_model_error',
                'error': str(e)
            }

class EnhancedUnifiedPostgresOllamaAgent:
    """Main agent class that provides the interface for the chatbot system"""
    
    def __init__(self):
        try:
            # Initialize the dual-model system
            self.dual_model_ai = DualModelDynamicAISystem()
            
            # Initialize basic stats
            self.stats = {
                'total_queries': 0,
                'successful_queries': 0,
                'failed_queries': 0,
                'avg_response_time': 0.0
            }
            
            logger.info("Enhanced Unified PostgreSQL Ollama Agent initialized successfully")
            logger.info("âœ… Dual-Model Dynamic AI System ready")
            
        except Exception as e:
            logger.error(f"Failed to initialize Enhanced Agent: {e}")
            raise
    
    async def process_any_question(self, question: str, tenant_id: str) -> Dict[str, Any]:
        """Main entry point for question processing"""
        try:
            # Update stats
            self.stats['total_queries'] += 1
            
            # Process using dual-model system
            result = await self.dual_model_ai.process_any_question(question, tenant_id)
            
            # Update stats based on result
            if result.get('success'):
                self.stats['successful_queries'] += 1
            else:
                self.stats['failed_queries'] += 1
            
            # Update average response time
            if 'processing_time' in result:
                total_time = self.stats['avg_response_time'] * (self.stats['total_queries'] - 1) + result['processing_time']
                self.stats['avg_response_time'] = total_time / self.stats['total_queries']
            
            return result
            
        except Exception as e:
            self.stats['failed_queries'] += 1
            logger.error(f"Question processing failed: {e}")
            
            return {
                'answer': f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸„à¸³à¸–à¸²à¸¡: {str(e)}\n\nà¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡",
                'success': False,
                'sql_query': None,
                'results_count': 0,
                'ai_system_used': 'error_handler',
                'error': str(e)
            }
    
    async def process_enhanced_question(self, question: str, tenant_id: str) -> Dict[str, Any]:
        """Compatibility method - redirects to process_any_question"""
        return await self.process_any_question(question, tenant_id)
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Get comprehensive system statistics with performance metrics"""
        return {
            'agent_stats': self.stats,
            'dual_model_stats': self.dual_model_ai.stats,
            'performance_metrics': {
                'avg_response_time': self.dual_model_ai.stats.get('avg_response_time', 0),
                'cache_hit_rate': self.dual_model_ai.stats.get('cache_hits', 0) / max(self.dual_model_ai.stats.get('sql_generated', 1), 1) * 100,
                'success_rate': self.dual_model_ai.stats.get('successful_queries', 0) / max(self.dual_model_ai.stats.get('total_queries', 1), 1) * 100,
                'sql_cache_size': len(self.dual_model_ai.sql_cache),
                'optimization_enabled': self.dual_model_ai.enable_parallel
            },
            'system_info': {
                'sql_model': self.dual_model_ai.SQL_MODEL,
                'nl_model': self.dual_model_ai.NL_MODEL,
                'ollama_url': self.dual_model_ai.ollama_client.base_url,
                'timeout_settings': {
                    'sql_timeout': self.dual_model_ai.timeout_sql,
                    'nl_timeout': self.dual_model_ai.timeout_nl
                }
            }
        }
    
    def clear_cache(self):
        """Clear SQL and schema cache for fresh start"""
        self.dual_model_ai.sql_cache.clear()
        self.dual_model_ai.schema_cache.clear()
        logger.info("Dual-Model caches cleared")

# =============================================================================
# EXPORT FOR BACKWARDS COMPATIBILITY
# =============================================================================

# These aliases ensure compatibility with different import patterns
UnifiedEnhancedPostgresOllamaAgent = EnhancedUnifiedPostgresOllamaAgent
DualModelDynamicAgent = EnhancedUnifiedPostgresOllamaAgent